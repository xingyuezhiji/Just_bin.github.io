<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://xingyuezhiji.github.io/</id>
    <title>Just_bin&apos;s Blog</title>
    <updated>2019-06-12T12:19:06.942Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://xingyuezhiji.github.io/"/>
    <link rel="self" href="https://xingyuezhiji.github.io//atom.xml"/>
    <subtitle>Love Life!</subtitle>
    <logo>https://xingyuezhiji.github.io//images/avatar.png</logo>
    <icon>https://xingyuezhiji.github.io//favicon.ico</icon>
    <rights>All rights reserved 2019, Just_bin&apos;s Blog</rights>
    <entry>
        <title type="html"><![CDATA[深度学习]]></title>
        <id>https://xingyuezhiji.github.io//post/shen-du-xue-xi</id>
        <link href="https://xingyuezhiji.github.io//post/shen-du-xue-xi">
        </link>
        <updated>2019-06-12T12:13:44.000Z</updated>
        <content type="html"><![CDATA[<p>BatchNormalization的作用</p>
<p>参考回答：</p>
<p>神经网络在训练的时候随着网络层数的加深,激活函数的输入值的整体分布逐渐往激活函数的取值区间上下限靠近,从而导致在反向传播时低层的神经网络的梯度消失。而BatchNormalization的作用是通过规范化的手段,将越来越偏的分布拉回到标准化的分布,使得激活函数的输入值落在激活函数对输入比较敏感的区域,从而使梯度变大,加快学习收敛速度,避免梯度消失的问题。</p>
<p>● 梯度消失
参考回答：
在神经网络中，当前面隐藏层的学习速率低于后面隐藏层的学习速率，即随着隐藏层数目的增加，分类准确率反而下降了。这种现象叫做消失的梯度问题。
● 循环神经网络，为什么好?
参考回答：
循环神经网络模型（RNN）是一种节点定向连接成环的人工神经网络，是一种反馈神经网络，RNN利用内部的记忆来处理任意时序的输入序列，并且在其处理单元之间既有内部的反馈连接又有前馈连接，这使得RNN可以更加容易处理不分段的文本等。
● 什么是Group Convolution
参考回答：
若卷积神将网络的上一层有N个卷积核,则对应的通道数也为N。设群数目为M,在进行卷积操作的时候,将通道分成M份,每个group对应N/M个通道,然后每个group卷积完成后输出叠在一起,作为当前层的输出通道。
● 什么是RNN
参考回答：
一个序列当前的输出与前面的输出也有关,在RNN网络结构中中,隐藏层的输入不仅包括输入层的输出还包含上一时刻隐藏层的输出,网络会对之前的信息进行记忆并应用于当前的输入计算中。
● 训练过程中,若一个模型不收敛,那么是否说明这个模型无效?导致模型不收敛的原因有哪些?
参考回答：
并不能说明这个模型无效,导致模型不收敛的原因可能有数据分类的标注不准确,样本的信息量太大导致模型不足以fit整个样本空间。学习率设置的太大容易产生震荡,太小会导致不收敛。可能复杂的分类任务用了简单的模型。数据没有进行归一化的操作。
● 图像处理中锐化和平滑的操作
参考回答：
锐化就是通过增强高频分量来减少图像中的模糊,在增强图像边缘的同时也增加了图像的噪声。
平滑与锐化相反,过滤掉高频分量,减少图像的噪声是图片变得模糊。</p>
<p>● VGG使用3<em>3卷积核的优势是什么?
参考回答：
2个3</em>3的卷积核串联和5<em>5的卷积核有相同的感知野,前者拥有更少的参数。多个3</em>3的卷积核比一个较大尺寸的卷积核有更多层的非线性函数,增加了非线性表达,使判决函数更具有判决性。
● Relu比Sigmoid的效果好在哪里?
参考回答：
Sigmoid的导数只有在0的附近时有较好的激活性,而在正负饱和区域的梯度趋向于0,从而产生梯度弥散的现象,而relu在大于0的部分梯度为常数,所以不会有梯度弥散现象。Relu的导数计算的更快。Relu在负半区的导数为0,所以神经元激活值为负时,梯度为0,此神经元不参与训练,具有稀疏性。
● 问题：神经网络中权重共享的是？
参考回答：
卷积神经网络、循环神经网络
解析：通过网络结构直接解释</p>
<p>● 问题：神经网络激活函数？
参考回答：
sigmod、tanh、relu
解析：需要掌握函数图像，特点，互相比较，优缺点以及改进方法</p>
<p>● 问题：在深度学习中，通常会finetuning已有的成熟模型，再基于新数据，修改最后几层神经网络权值，为什么？
参考回答：
实践中的数据集质量参差不齐，可以使用训练好的网络来进行提取特征。把训练好的网络当做特征提取器。
● 问题：画GRU结构图
参考回答：</p>
<p>GRU有两个门：更新门，输出门
解析：如果不会画GRU，可以画LSTM或者RNN。再或者可以讲解GRU与其他两个网络的联系和区别。不要直接就说不会。</p>
<p>● Attention机制的作用
参考回答：
减少处理高维输入数据的计算负担,结构化的选取输入的子集,从而降低数据的维度。让系统更加容易的找到输入的数据中与当前输出信息相关的有用信息,从而提高输出的质量。帮助类似于decoder这样的模型框架更好的学到多种内容模态之间的相互关系。
● Lstm和Gru的原理
参考回答：
Lstm由输入门,遗忘门,输出门和一个cell组成。第一步是决定从cell状态中丢弃什么信息,然后在决定有多少新的信息进入到cell状态中,最终基于目前的cell状态决定输出什么样的信息。
Gru由重置门和跟新门组成,其输入为前一时刻隐藏层的输出和当前的输入,输出为下一时刻隐藏层的信息。重置门用来计算候选隐藏层的输出,其作用是控制保留多少前一时刻的隐藏层。跟新门的作用是控制加入多少候选隐藏层的输出信息,从而得到当前隐藏层的输出。</p>
<p>● 什么是dropout
参考回答：
在神经网络的训练过程中,对于神经单元按一定的概率将其随机从网络中丢弃,从而达到对于每个mini-batch都是在训练不同网络的效果,防止过拟合。
● LSTM每个门的计算公式
参考回答：
遗忘门:
输入门:</p>
<p>输出门:</p>
<p>● DropConnect的原理
参考回答：
防止过拟合方法的一种,与dropout不同的是,它不是按概率将隐藏层的节点输出清0,而是对每个节点与之相连的输入权值以一定的概率清0。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[平均数编码：针对高基数定性特征（类别特征）的数据预处理/特征工程]]></title>
        <id>https://xingyuezhiji.github.io//post/ping-jun-shu-bian-ma-zhen-dui-gao-ji-shu-ding-xing-te-zheng-lei-bie-te-zheng-de-shu-ju-yu-chu-li-te-zheng-gong-cheng</id>
        <link href="https://xingyuezhiji.github.io//post/ping-jun-shu-bian-ma-zhen-dui-gao-ji-shu-ding-xing-te-zheng-lei-bie-te-zheng-de-shu-ju-yu-chu-li-te-zheng-gong-cheng">
        </link>
        <updated>2019-03-14T02:43:27.000Z</updated>
        <content type="html"><![CDATA[<p>平均数编码：针对高基数定性特征（类别特征）的数据预处理/特征工程
光喻
光喻
机器学习/因果推断
​关注他
鱼遇雨欲语与余
等 182 人赞同了该文章
（在另一篇文章中，我正在汇总所有已知的数据挖掘特征工程技巧：【持续更新】机器学习特征工程实用技巧大全 - 知乎专栏。）</p>
<p>前言</p>
<p>读完sklearn.preprocessing所有函数的API文档之后，基础的特征工程就可以算是入门了。然而，进阶的特征工程往往依赖于数据分析师的直觉与经验，而且与具体的数据有密切的联系，比较难找到系统性的“最好”的特征工程方法。</p>
<p>在这里，我希望能向大家分享一种极其有效的、针对高基数定性特征（类别特征）的数据预处理方法。在各类竞赛中，有许多人使用这种方法取得了非常优秀的成绩，但是中文网络上似乎还没有人对此做过介绍。</p>
<p>平均数编码：针对高基数定性特征（类别特征）的数据预处理
Mean Encoding: A Preprocessing Scheme for High-Cardinality Categorical Features
（论文原文下载：http://helios.mm.di.uoa.gr/~rouvas/ssi/sigkdd/sigkdd.vol3.1/barreca.pdf，感谢评论区@jin zhang提供更清晰的pdf版本）</p>
<p>如果某一个特征是定性的（categorical），而这个特征的可能值非常多（高基数），那么平均数编码（mean encoding）是一种高效的编码方式。在实际应用中，这类特征工程能极大提升模型的性能。</p>
<p>在机器学习与数据挖掘中，不论是分类问题（classification）还是回归问题（regression），采集的数据常常会包括定性特征（categorical feature）。因为定性特征表示某个数据属于一个特定的类别，所以在数值上，定性特征值通常是从0到n的离散整数。例子：花瓣的颜色（红、黄、蓝）、性别（男、女）、地址、某一列特征是否存在缺失值（这种NA 指示列常常会提供有效的额外信息）。</p>
<p>一般情况下，针对定性特征，我们只需要使用sklearn的OneHotEncoder或LabelEncoder进行编码：（data_df是一个pandas dataframe，每一行是一个training example，每一列是一个特征。在这里我们假设&quot;street_address&quot;是一个字符类的定性特征。）</p>
<p>from sklearn.preprocessing import OneHotEncoder, LabelEncoder
import numpy as np
import pandas as pd</p>
<p>le = LabelEncoder()
data_df['street_address'] = le.fit_transform(data_df['street_address'])</p>
<p>ohe = OneHotEncoder(n_values='auto', categorical_features='all', dtype=np.float64, sparse=True, handle_unknown='error')
one_hot_matrix = ohe.fit_transform(data_df['street_address'])
LabelEncoder能够接收不规则的特征列，并将其转化为从0到n-1的整数值（假设一共有n种不同的类别）；OneHotEncoder则能通过哑编码，制作出一个m*n的稀疏矩阵（假设数据一共有m行，具体的输出矩阵格式是否稀疏可以由sparse参数控制）。</p>
<p>更详细的API文档参见：sklearn.preprocessing.LabelEncoder - scikit-learn 0.18.1 documentation以及sklearn.preprocessing.OneHotEncoder - scikit-learn 0.18.1 documentation</p>
<p>这类简单的预处理能够满足大多数数据挖掘算法的需求。</p>
<p>值得一提的是，LabelEncoder将n种类别编码为从0到n-1的整数，虽然能够节省内存和降低算法的运行时间，但是隐含了一个假设：不同的类别之间，存在一种顺序关系。在具体的代码实现里，LabelEncoder会对定性特征列中的所有独特数据进行一次排序，从而得出从原始输入到整数的映射。</p>
<p>定性特征的基数（cardinality）指的是这个定性特征所有可能的不同值的数量。在高基数（high cardinality）的定性特征面前，这些数据预处理的方法往往得不到令人满意的结果。</p>
<p>高基数定性特征的例子：IP地址、电子邮件域名、城市名、家庭住址、街道、产品号码。</p>
<p>主要原因：</p>
<p>LabelEncoder编码高基数定性特征，虽然只需要一列，但是每个自然数都具有不同的重要意义，对于y而言线性不可分。使用简单模型，容易欠拟合（underfit），无法完全捕获不同类别之间的区别；使用复杂模型，容易在其他地方过拟合（overfit）。
OneHotEncoder编码高基数定性特征，必然产生上万列的稀疏矩阵，易消耗大量内存和训练时间，除非算法本身有相关优化（例：SVM）。</p>
<p>因此，我们可以尝试使用平均数编码（mean encoding）的编码方法，在贝叶斯的架构下，利用所要预测的应变量（target variable），有监督地确定最适合这个定性特征的编码方式。在Kaggle的数据竞赛中，这也是一种常见的提高分数的手段。</p>
<p>基本思路与原理：</p>
<p>平均数编码是一种有监督（supervised）的编码方式，适用于分类和回归问题。为了简化讨论，以下的所有代码都以分类问题作为例子。</p>
<p>假设在分类问题中，目标y一共有C个不同类别，具体的一个类别用target表示；某一个定性特征variable一共有K个不同类别，具体的一个类别用k表示。</p>
<p>先验概率（prior）：数据点属于某一个target（y）的概率，P(y = target)</p>
<p>后验概率（posterior）：该定性特征属于某一类时，数据点属于某一个target（y）的概率，P(target = y | variable = k)</p>
<p>本算法的基本思想：将variable中的每一个k，都表示为（估算的）它所对应的目标y值概率：</p>
<p>\hat{P} (target = y | variable = k)。（估算的结果都用“^”表示，以示区分）
（备注）因此，整个数据集将增加（C-1）列。是C-1而不是C的原因：
\sum_{i}^{}{\hat{P}(target = y_i | variable = k)} =1，所以最后一个y_i的概率值必然和其他y_i的概率值线性相关。在线性模型、神经网络以及SVM里，不能加入线性相关的特征列。如果你使用的是基于决策树的模型（gbdt、rf等），个人仍然不推荐这种over-parameterization。</p>
<p>先验概率与后验概率的计算：</p>
<p>因为我们没有上帝视角，所以我们在计算中，需要利用已有数据估算先验概率和后验概率。我们在此使用的具体方法被称为Empirical Bayes（Empirical Bayes method）。和一般的贝叶斯方法（如常见的Laplace Smoothing）不同，我们在估算先验概率时，会使用已知数据的平均值，而不是\frac{1}{C} 。</p>
<p>接下来的计算就是简单的统计：</p>
<p>\hat{P}(y = target) = (y = target)的数量 / y 的总数</p>
<p>\hat{P}(target = y | variable = k) = (y = target 并且 variable = k)的数量 / (variable = k)的数量</p>
<p>（其实我本来可以把公式用sigma求和写出来的，但是那样不太方便直观理解）</p>
<p>使用不同的统计方法，以上两个公式的计算方法也会不同。
权重：</p>
<p>我们已经得到了先验概率估计\hat{P}(y = target)和后验概率估计\hat{P}(target = y | variable = k)。最终编码所使用的概率估算，应当是先验概率与后验概率的一个凸组合（convex combination）。由此，我们引入先验概率的权重\lambda 来计算编码所用概率\hat{P}：</p>
<p>\hat{P} = \lambda * \hat{P}(y = target) + (1 - \lambda) * \hat{P}(target = y | variable = k)
或：\hat{P} = \lambda * prior + (1 - \lambda) * posterior</p>
<p>直觉上，\lambda 应该具有以下特性：</p>
<p>如果测试集中出现了新的特征类别（未在训练集中出现），那么\lambda = 1。
一个特征类别在训练集内出现的次数越多，后验概率的可信度越高，其权重也越大。</p>
<p>在贝叶斯统计学中，\lambda 也被称为shrinkage parameter。
权重函数：</p>
<p>我们需要定义一个权重函数，输入是特征类别在训练集中出现的次数n，输出是对于这个特征类别的先验概率的权重\lambda。假设一个特征类别的出现次数为n，以下是一个常见的权重函数：</p>
<p>\lambda(n) = \frac{1}{1 + e^{(n - k)/f}}</p>
<p>这个函数有两个参数：
k：当n = k时，\lambda = 0.5，先验概率与后验概率的权重相同；当n &gt; k时，\lambda &lt; 0.5。</p>
<p>f：控制函数在拐点附近的斜率，f越大，“坡”越缓。</p>
<p>图示：k=1时，不同的f对于函数图象的影响。</p>
<p>当(freq_col - k) / f太大的时候，np.exp可能会产生overflow的警告。我们不需要管这个警告，因为某一类别的频数极高，分母无限时，最终先验概率的权重将成为0，这也表示我们对于后验概率有充足的信任。</p>
<p>延伸</p>
<p>以上的算法设计能解决多个（&gt;2）类别的分类问题，自然也能解决更简单的2类分类问题以及回归问题。</p>
<p>还有一种情况：定性特征本身包括了不同级别。例如，国家包含了省，省包含了市，市包含了街区。有些街区可能就包含了大量的数据点，而有些省却可能只有稀少的几个数据点。这时，我们的解决方法是，在empirical bayes里加入不同层次的先验概率估计。</p>
<p>代码实现</p>
<p>原论文并没有提到，如果fit时使用了全部的数据，transform时也使用了全部数据，那么之后的机器学习模型会产生过拟合。因此，我们需要将数据分层分为n_splits个fold，每一个fold的数据都是利用剩下的(n_splits - 1)个fold得出的统计数据进行转换。n_splits越大，编码的精度越高，但也更消耗内存和运算时间。</p>
<p>编码完毕后，是否删除原始特征列，应当具体问题具体分析。</p>
<p>附：完整版MeanEncoder代码（python）。</p>
<p>一个MeanEncoder对象可以提供fit_transform和transform方法，不支持fit方法，暂不支持训练时的sample_weight参数。</p>
<pre><code>
import numpy as np
import pandas as pd
from sklearn.model_selection import StratifiedKFold
from itertools import product

class MeanEncoder:
    def __init__(self, categorical_features, n_splits=5, target_type='classification', prior_weight_func=None):
        &quot;&quot;&quot;
        :param categorical_features: list of str, the name of the categorical columns to encode

        :param n_splits: the number of splits used in mean encoding

        :param target_type: str, 'regression' or 'classification'

        :param prior_weight_func:
        a function that takes in the number of observations, and outputs prior weight
        when a dict is passed, the default exponential decay function will be used:
        k: the number of observations needed for the posterior to be weighted equally as the prior
        f: larger f --&gt; smaller slope
        &quot;&quot;&quot;

        self.categorical_features = categorical_features
        self.n_splits = n_splits
        self.learned_stats = {}

        if target_type == 'classification':
            self.target_type = target_type
            self.target_values = []
        else:
            self.target_type = 'regression'
            self.target_values = None

        if isinstance(prior_weight_func, dict):
            self.prior_weight_func = eval('lambda x: 1 / (1 + np.exp((x - k) / f))', dict(prior_weight_func, np=np))
        elif callable(prior_weight_func):
            self.prior_weight_func = prior_weight_func
        else:
            self.prior_weight_func = lambda x: 1 / (1 + np.exp((x - 2) / 1))

    @staticmethod
    def mean_encode_subroutine(X_train, y_train, X_test, variable, target, prior_weight_func):
        X_train = X_train[[variable]].copy()
        X_test = X_test[[variable]].copy()

        if target is not None:
            nf_name = '{}_pred_{}'.format(variable, target)
            X_train['pred_temp'] = (y_train == target).astype(int)  # classification
        else:
            nf_name = '{}_pred'.format(variable)
            X_train['pred_temp'] = y_train  # regression
        prior = X_train['pred_temp'].mean()

        col_avg_y = X_train.groupby(by=variable, axis=0)['pred_temp'].agg({'mean': 'mean', 'beta': 'size'})
        col_avg_y['beta'] = prior_weight_func(col_avg_y['beta'])
        col_avg_y[nf_name] = col_avg_y['beta'] * prior + (1 - col_avg_y['beta']) * col_avg_y['mean']
        col_avg_y.drop(['beta', 'mean'], axis=1, inplace=True)

        nf_train = X_train.join(col_avg_y, on=variable)[nf_name].values
        nf_test = X_test.join(col_avg_y, on=variable).fillna(prior, inplace=False)[nf_name].values

        return nf_train, nf_test, prior, col_avg_y

    def fit_transform(self, X, y):
        &quot;&quot;&quot;
        :param X: pandas DataFrame, n_samples * n_features
        :param y: pandas Series or numpy array, n_samples
        :return X_new: the transformed pandas DataFrame containing mean-encoded categorical features
        &quot;&quot;&quot;
        X_new = X.copy()
        if self.target_type == 'classification':
            skf = StratifiedKFold(self.n_splits)
        else:
            skf = KFold(self.n_splits)

        if self.target_type == 'classification':
            self.target_values = sorted(set(y))
            self.learned_stats = {'{}_pred_{}'.format(variable, target): [] for variable, target in
                                  product(self.categorical_features, self.target_values)}
            for variable, target in product(self.categorical_features, self.target_values):
                nf_name = '{}_pred_{}'.format(variable, target)
                X_new.loc[:, nf_name] = np.nan
                for large_ind, small_ind in skf.split(y, y):
                    nf_large, nf_small, prior, col_avg_y = MeanEncoder.mean_encode_subroutine(
                        X_new.iloc[large_ind], y.iloc[large_ind], X_new.iloc[small_ind], variable, target, self.prior_weight_func)
                    X_new.iloc[small_ind, -1] = nf_small
                    self.learned_stats[nf_name].append((prior, col_avg_y))
        else:
            self.learned_stats = {'{}_pred'.format(variable): [] for variable in self.categorical_features}
            for variable in self.categorical_features:
                nf_name = '{}_pred'.format(variable)
                X_new.loc[:, nf_name] = np.nan
                for large_ind, small_ind in skf.split(y, y):
                    nf_large, nf_small, prior, col_avg_y = MeanEncoder.mean_encode_subroutine(
                        X_new.iloc[large_ind], y.iloc[large_ind], X_new.iloc[small_ind], variable, None, self.prior_weight_func)
                    X_new.iloc[small_ind, -1] = nf_small
                    self.learned_stats[nf_name].append((prior, col_avg_y))
        return X_new

    def transform(self, X):
        &quot;&quot;&quot;
        :param X: pandas DataFrame, n_samples * n_features
        :return X_new: the transformed pandas DataFrame containing mean-encoded categorical features
        &quot;&quot;&quot;
        X_new = X.copy()

        if self.target_type == 'classification':
            for variable, target in product(self.categorical_features, self.target_values):
                nf_name = '{}_pred_{}'.format(variable, target)
                X_new[nf_name] = 0
                for prior, col_avg_y in self.learned_stats[nf_name]:
                    X_new[nf_name] += X_new[[variable]].join(col_avg_y, on=variable).fillna(prior, inplace=False)[
                        nf_name]
                X_new[nf_name] /= self.n_splits
        else:
            for variable in self.categorical_features:
                nf_name = '{}_pred'.format(variable)
                X_new[nf_name] = 0
                for prior, col_avg_y in self.learned_stats[nf_name]:
                    X_new[nf_name] += X_new[[variable]].join(col_avg_y, on=variable).fillna(prior, inplace=False)[
                        nf_name]
                X_new[nf_name] /= self.n_splits

        return X_new
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[高级特征工程I]]></title>
        <id>https://xingyuezhiji.github.io//post/gao-ji-te-zheng-gong-cheng-i</id>
        <link href="https://xingyuezhiji.github.io//post/gao-ji-te-zheng-gong-cheng-i">
        </link>
        <updated>2019-03-06T15:04:48.000Z</updated>
        <content type="html"><![CDATA[<p>高级特征工程I﻿
Mean encodings
以下是Coursera上的How to Win a Data Science Competition: Learn from Top Kagglers课程笔记。
学习目标</p>
<p>Regularize mean encodings
Extend mean encodings
Summarize the concept of mean encodings
Concept of mean encoding
均值编码是一种非常强大的技术，它有很多名字，例如:likelihood encoding、target encoding，但这里我们叫它均值编码。我们举一个二分类任务的例子。
feature	feature_label	feature_mean	target
0	Moscow	1	0.4	0
1	Moscow	1	0.4	1
2	Moscow	1	0.4	1
3	Moscow	1	0.4	0
4	Moscow	1	0.4	0
5	Tver	2	0.8	1
6	Tver	2	0.8	1
7	Tver	2	0.8	1
8	Tver	2	0.8	0
9	Klin	0	0.0	0
10	klin	0	0.0	0
11	Tver	2	1	1
我们想对feature变量进行编码，最直接、常用的方式就是label encoding，这就是第二列数据。</p>
<p>平均编码以不同的方式去完成这个任务，它用每个城市自身对应的目标均值来进行编码。例如，对于Moscow，我们有五行，三个0和两个1。 所以我们用2除以5或0.4对它进行编码。用同样的方法处理其他城市。
现在了解一下细节。当我们的数据集非常大，包含数百个不同的城市，让我们试着比较一下。我们绘制了0,1 class的直方图。
label_encoding.jpg
在label encoding的情况下，我们得到的图看起来没有任何逻辑顺序。
mean_encoding.jpg
但是当我们使用mean encoding对目标进行编码时，类看起来更加可分了，像是被排序过。
一般来说，模型对复杂、非线性的特征目标越依赖，均值编码越有效。例如树模型的深度有限，可以用平均编码来补偿它，可以用它的短板来获得更好的分数。
以上只是一个例子，传递的是一种思想，实际上可以做很多类似的操作。
Ways to use target variable</p>
<p>Goods-number of ones in a group,</p>
<p>Bads-number of zeros
Likelihood=GoodsGoods+Bads=mean(target)Likelihood=GoodsGoods+Bads=mean(target)
WeightofEvidence=ln(GoodsBads)∗100WeightofEvidence=ln⁡(GoodsBads)∗100
Count=Goods=sum(target)Count=Goods=sum(target)
Diff=Goods−BadsDiff=Goods−Bads
构造Mean encoding的例子</p>
<p>
1
123</p>
<p>
1
means=X_tr.groupby(col).target.mean()train_new[col+'_mean_target']
=train_new[col].map(means)val_new[col+'_mean_target']=val_new[col].map
(means)
将它运用到模型中，出现了严重的过拟合，但是为什么呢？
Train
feature	feature_label	feature_mean	target
8	Tver	2	0.8	0
9	Klin	0	0.0	0
Validation
feature	feature_label	feature_mean	target
10	klin	0	0.0	0
11	Tver	2	1	1
When they are categorized, it’s pretty common to get results like in an example, target 0 in train and target 1 in validation. Mean encodings turns into a perfect feature for such categories. That’s why we immediately get very good scores on train and fail hardly on validation.
Regularization
在上一节，我们意识到平均编码不能按原样使用，需要对训练数据进行某种正规化。现在我们将实施四种不同的正则化方法。
1.CV loop inside training data;
2.Smoothing;
3.Adding random noise;
4.Sorting and calculating expanding mean.
Conclusion</p>
<p>There are a lot ways to regularize mean encodings
Unending battle with target variable leakage
CV loop or Expanding mean for partical tasks.
1.KFold scheme</p>
<p>kfold.jpg
通常做四到五折的交叉验证就能得到不错的结果，无序调整此数字。
代码例子
kfold_code.jpg
这个方法看起来已经完全避免了目标变量的泄露，但事实并非如此。</p>
<p>这里我们通过留一法对Moscow进行编码
feature	feature_mean	target
0	Moscow	0.50	0
1	Moscow	0.25	1
2	Moscow	0.25	1
3	Moscow	0.50	0
4	Moscow	0.50	0
对于第一行，我们得到0.5，因为有两个1和 其余行中有两个0。 同样，对于第二行，我们得到0.25，依此类推。 但仔细观察，所有结果和由此产生的特征。 它完美地分割数据，具有等于或等的特征的行 大于0.5的目标为0，其余行的目标为1。 我们没有明确使用目标变量，但我们的编码是有偏置的。</p>
<p>目标变量的泄露效果对于KFold scheme仍然是有效的，只是效果温和了点。</p>
<p>在实践中，如果您有足够的数据并使用四或五折，编码将通过这种正规化策略正常工作。 只是要小心并使用正确的验证。
2.Smoothing</p>
<p>Alpha controls the amount of regularization
Only works together with some other regularization method</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mfrac><mrow><mi>m</mi><mi>e</mi><mi>a</mi><mi>n</mi><mo>(</mo><mi>t</mi><mi>a</mi><mi>r</mi><mi>g</mi><mi>e</mi><mi>t</mi><mo>)</mo><mi>n</mi><mi>r</mi><mi>o</mi><mi>w</mi><mi>s</mi><mo>+</mo><mi>g</mi><mi>l</mi><mi>o</mi><mi>b</mi><mi>a</mi><mi>l</mi><mi>m</mi><mi>e</mi><mi>a</mi><mi>n</mi><mi>a</mi><mi>l</mi><mi>p</mi><mi>h</mi><mi>a</mi></mrow><mrow><mi>n</mi><mi>r</mi><mi>o</mi><mi>w</mi><mi>s</mi><mo>+</mo><mi>a</mi><mi>l</mi><mi>p</mi><mi>h</mi><mi>a</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">\frac{mean(target)nrows + globalmeanalpha}{nrows+alpha}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:2.30744em;vertical-align:-0.8804400000000001em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.427em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">n</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mord mathdefault">o</span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mord mathdefault">s</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord mathdefault">a</span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mord mathdefault">p</span><span class="mord mathdefault">h</span><span class="mord mathdefault">a</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">m</span><span class="mord mathdefault">e</span><span class="mord mathdefault">a</span><span class="mord mathdefault">n</span><span class="mopen">(</span><span class="mord mathdefault">t</span><span class="mord mathdefault">a</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="mord mathdefault">e</span><span class="mord mathdefault">t</span><span class="mclose">)</span><span class="mord mathdefault">n</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mord mathdefault">o</span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mord mathdefault">s</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mord mathdefault">o</span><span class="mord mathdefault">b</span><span class="mord mathdefault">a</span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mord mathdefault">m</span><span class="mord mathdefault">e</span><span class="mord mathdefault">a</span><span class="mord mathdefault">n</span><span class="mord mathdefault">a</span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mord mathdefault">p</span><span class="mord mathdefault">h</span><span class="mord mathdefault">a</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.8804400000000001em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p>
<p>它具有控制正则化量的超参数alpha。 当alpha为零时，我们没有正则化，并且当alpha接近无穷大时，一切都变成了globalmean。
在某种意义上，alpha等于我们可以信任的类别大小。也可以使用其他一些公式，基本上任何惩罚编码类别的东西都可以被认为是smoothing。
3.Nosie</p>
<p>Noise degrades the quality of encoding
通过添加噪声，会降低训练数据的编码质量。这种方法很不稳定，很难使它工作。主要问题在于我们需要添加的噪声量。
How much noise should we add?
太多的噪声会把这个特征变成垃圾，虽然噪声太小意味着更正规化。你需要努力地微调它。
Usually used together with LOO(Leave one out).
这种方法通常与LOO正则化一起使用。如果你没有很多时间，它可能不是最好选择。
4.Expanding mean</p>
<p>Least amount of leakage
No hyper parameters
Irregular encoding quality
Built-in in CatBoost.
代码例子</p>
<p>
1
123</p>
<p>
1
cumsum=df_tr.groupby(col)['target'].cumsum()-df_tr['target']cumcnt=df_tr
.groupby(col).cumcount()train_new[col+'_mean_target']=cusum/cumcnt</p>
<p>cumsum存储目标变量的累计和，直到给定行，cumcnt存储累积计数。该方法引入的目标变量的泄漏量最少，唯一的缺点是特征质量不均匀。但这不是什么大不了的事，我们可以从不同的数据排列计算编码的平均模型。
它被用于CatBoost库中，证明了它在分类数据集上表现非常出色。
Extensions and generalizations
如何在回归和多分类任务中进行Mean encoding
如何将编码应用于具有多对多关系的领域
我们可以根据时间序列中的目标构建哪些功能
编码交互和数字特征
Many-to-many relations</p>
<p>原始数据
User_id	APPS	Target
10	APP1;APP2;APP3	0
11	APP4;APP1	1
12	APP2	1
100	APP3;APP9	0
现在考虑一个例子，基于用在智能手机上已装的APP，预测它是否会安装，这是一个二分类任务。从表中数据可知，每个用户可能有多个应用程序，每个应用程序由多个用户使用，因此这是多对多的关系。而麻烦在于，如何从多对多的关系中提取均值。
长数据表示
User_id	APP_id	Target
10	APP1	0
10	APP2	0
10	APP3	0
11	APP4	1
11	APP1	1
把原始数据转为长数据表示，如上表。使用此表，我们可以自然地计算APP的均值编码。但是如何将其映射回用户呢？
每个用户都有许多APP，但不都是“APP1,APP2,APP3”。因此我们用向量表示(0.1,0.2,0.1)，我们还可以从向量中收集各种统计数据，比如均值、标准差、最大最小值等等。
Time series</p>
<p>Time structure allows us to make a lot of complicated features.
Rolling statistics of target variable.
一方面，这是一种限制，另一方面，它允许我们只做一些复杂的特征。考虑一个例子：
Day	User	Spend	Amount	Prev_user	Prev_spend_avg
1	101	FOOD	2.0	0.0	0.0
1	101	GAS	4.0	0.0	0.0
1	102	FOOD	3.0	0.0	0.0
2	101	GAS	4.0	6.0	4.0
2	101	TV	8.0	6.0	0.0
2	102	FOOD	2.0	3.0	2.5
我们需要预测用户会为哪个类别花钱。 我们有两天的时间，两个用户， 和三个支出类别。 一些好的特征是用户在前一天消费总额，所有用户在给定类别中花费的平均金额。 因此，在第1天，用户101花费6美元，用户102花费$3。 因此，我们认为这些数字是第2天的未来值。 同样，可以按类别划分平均金额。
我们拥有的数据越多，可以创造的特征就越复杂。
Interactions and numerical features</p>
<p>Analyzing fitted model
Binning numeric and selecting interactions
在实践中，通常需要编码数字特征以及进行特征组合。要对数字特征进行编码，我们只需要对其进行分区，然后将其视为分类。我们以没有进行任何编码的原始特征和决策树模型为例。
interaction_tree.png
如何为数字特征分组？
如果数字特征有很多分裂点，则表示它于目标有一些复杂的依赖，并且试图去编码它。此外这些精确的分裂点可用于对特征进行分类，所以通过分析模型结构，我们既可以识别这些可疑的数字特征，又可以找到很好的方法去给它分组。
如何挑选特征组合？
先看决策树中如何提取交互特征。参照上图，如果两个特征在相邻的节点中，则这两个特征在相互作用。考虑到这点，我们可以遍历模型中的所有树，计算每个特征组合出现的次数。最常见的组合可能值得进行均值编码。</p>
<p>例如，如果我们发现feature1和feature2这一对特征最常见，我们可以在数据中连接这些特征，这意味编码产生交互。
Correct validation reminder</p>
<p>Local experiments:
Estimate encodings on X_tr
Map them to X_tr and X_val
Regularize on X_tr
Validate model on X_tr/X_val split
Submission:
Estimate encodings on whole Train data
Map them on Train and Test
Regularize on Train
Fit on Train
reminder_set.png
End
Main advantages:
Compact transformation of categorical variables
Powerful basis for feature engineering
Disadvantages:
Need careful validation, there a lot of ways to overfit
Significant improvements only on specific datasets</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hello Hve Notes]]></title>
        <id>https://xingyuezhiji.github.io//post/hello-hve-notes</id>
        <link href="https://xingyuezhiji.github.io//post/hello-hve-notes">
        </link>
        <updated>2018-12-11T16:00:00.000Z</updated>
        <content type="html"><![CDATA[<p>👏  欢迎使用 <strong>Hve Notes</strong> ！</p>
<p>Github: <a href="https://github.com/hve-notes/hve-notes">Hve Notes</a><br>
项目主页: <a href="http://hvenotes.fehey.com/">Hve Notes</a><br>
示例网站: <a href="http://fehey.com/">示例网站一</a> <a href="http://hve-notes.github.io">示例网站二</a></p>
<p>✍️  <strong>Hve Notes</strong> 一个静态博客写作客户端。你可以用它来记录你的生活、心情、知识、笔记、创意... ...</p>
<h2 id="特性">特性👇</h2>
<p>📝  你可以使用最酷的 <strong>Markdown</strong> 语法，进行快速创作</p>
<p>🌉  你可以给文章配上精美的封面图和在文章任意位置插入图片</p>
<p>🏷️  你可以对文章进行标签分组</p>
<p>📋  你可以自定义菜单，甚至可以创建外部链接菜单</p>
<p>💻  你可以在 <strong>𝖶𝗂𝗇𝖽𝗈𝗐𝗌</strong> 或 <strong>𝖬𝖺𝖼𝖮𝖲</strong> 设备上使用此客户端</p>
<p>🌎  你可以使用 <strong>𝖦𝗂𝗍𝗁𝗎𝖻 𝖯𝖺𝗀𝖾𝗌</strong> 或 <strong>Coding Pages</strong> 向世界展示，未来将支持更多平台</p>
<p>💬  你可以进行简单的配置，接入 <a href="https://github.com/gitalk/gitalk">Gitalk</a> 或 <a href="https://github.com/SukkaW/DisqusJS">DisqusJS</a> 评论系统</p>
<p>🇬🇧  你可以使用<strong>中文简体</strong>或<strong>英语</strong></p>
<p>🌁  你可以任意使用应用内默认主题或任意第三方主题</p>
<p>🌱 当然 <strong>Hve Notes</strong> 还很年轻，有很多不足，但请相信，它会不停向前🏃</p>
<p>未来，它一定会成为你离不开的伙伴</p>
<p>尽情发挥你的才华吧！</p>
<p>😘 Enjoy~</p>
]]></content>
    </entry>
</feed>