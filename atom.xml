<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://xingyuezhiji.github.io/</id>
    <title>Just_bin&apos;s Blog</title>
    <updated>2019-06-12T12:19:06.942Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://xingyuezhiji.github.io/"/>
    <link rel="self" href="https://xingyuezhiji.github.io//atom.xml"/>
    <subtitle>Love Life!</subtitle>
    <logo>https://xingyuezhiji.github.io//images/avatar.png</logo>
    <icon>https://xingyuezhiji.github.io//favicon.ico</icon>
    <rights>All rights reserved 2019, Just_bin&apos;s Blog</rights>
    <entry>
        <title type="html"><![CDATA[æ·±åº¦å­¦ä¹ ]]></title>
        <id>https://xingyuezhiji.github.io//post/shen-du-xue-xi</id>
        <link href="https://xingyuezhiji.github.io//post/shen-du-xue-xi">
        </link>
        <updated>2019-06-12T12:13:44.000Z</updated>
        <content type="html"><![CDATA[<p>BatchNormalizationçš„ä½œç”¨</p>
<p>å‚è€ƒå›ç­”ï¼š</p>
<p>ç¥ç»ç½‘ç»œåœ¨è®­ç»ƒçš„æ—¶å€™éšç€ç½‘ç»œå±‚æ•°çš„åŠ æ·±,æ¿€æ´»å‡½æ•°çš„è¾“å…¥å€¼çš„æ•´ä½“åˆ†å¸ƒé€æ¸å¾€æ¿€æ´»å‡½æ•°çš„å–å€¼åŒºé—´ä¸Šä¸‹é™é è¿‘,ä»è€Œå¯¼è‡´åœ¨åå‘ä¼ æ’­æ—¶ä½å±‚çš„ç¥ç»ç½‘ç»œçš„æ¢¯åº¦æ¶ˆå¤±ã€‚è€ŒBatchNormalizationçš„ä½œç”¨æ˜¯é€šè¿‡è§„èŒƒåŒ–çš„æ‰‹æ®µ,å°†è¶Šæ¥è¶Šåçš„åˆ†å¸ƒæ‹‰å›åˆ°æ ‡å‡†åŒ–çš„åˆ†å¸ƒ,ä½¿å¾—æ¿€æ´»å‡½æ•°çš„è¾“å…¥å€¼è½åœ¨æ¿€æ´»å‡½æ•°å¯¹è¾“å…¥æ¯”è¾ƒæ•æ„Ÿçš„åŒºåŸŸ,ä»è€Œä½¿æ¢¯åº¦å˜å¤§,åŠ å¿«å­¦ä¹ æ”¶æ•›é€Ÿåº¦,é¿å…æ¢¯åº¦æ¶ˆå¤±çš„é—®é¢˜ã€‚</p>
<p>â— æ¢¯åº¦æ¶ˆå¤±
å‚è€ƒå›ç­”ï¼š
åœ¨ç¥ç»ç½‘ç»œä¸­ï¼Œå½“å‰é¢éšè—å±‚çš„å­¦ä¹ é€Ÿç‡ä½äºåé¢éšè—å±‚çš„å­¦ä¹ é€Ÿç‡ï¼Œå³éšç€éšè—å±‚æ•°ç›®çš„å¢åŠ ï¼Œåˆ†ç±»å‡†ç¡®ç‡åè€Œä¸‹é™äº†ã€‚è¿™ç§ç°è±¡å«åšæ¶ˆå¤±çš„æ¢¯åº¦é—®é¢˜ã€‚
â— å¾ªç¯ç¥ç»ç½‘ç»œï¼Œä¸ºä»€ä¹ˆå¥½?
å‚è€ƒå›ç­”ï¼š
å¾ªç¯ç¥ç»ç½‘ç»œæ¨¡å‹ï¼ˆRNNï¼‰æ˜¯ä¸€ç§èŠ‚ç‚¹å®šå‘è¿æ¥æˆç¯çš„äººå·¥ç¥ç»ç½‘ç»œï¼Œæ˜¯ä¸€ç§åé¦ˆç¥ç»ç½‘ç»œï¼ŒRNNåˆ©ç”¨å†…éƒ¨çš„è®°å¿†æ¥å¤„ç†ä»»æ„æ—¶åºçš„è¾“å…¥åºåˆ—ï¼Œå¹¶ä¸”åœ¨å…¶å¤„ç†å•å…ƒä¹‹é—´æ—¢æœ‰å†…éƒ¨çš„åé¦ˆè¿æ¥åˆæœ‰å‰é¦ˆè¿æ¥ï¼Œè¿™ä½¿å¾—RNNå¯ä»¥æ›´åŠ å®¹æ˜“å¤„ç†ä¸åˆ†æ®µçš„æ–‡æœ¬ç­‰ã€‚
â— ä»€ä¹ˆæ˜¯Group Convolution
å‚è€ƒå›ç­”ï¼š
è‹¥å·ç§¯ç¥å°†ç½‘ç»œçš„ä¸Šä¸€å±‚æœ‰Nä¸ªå·ç§¯æ ¸,åˆ™å¯¹åº”çš„é€šé“æ•°ä¹Ÿä¸ºNã€‚è®¾ç¾¤æ•°ç›®ä¸ºM,åœ¨è¿›è¡Œå·ç§¯æ“ä½œçš„æ—¶å€™,å°†é€šé“åˆ†æˆMä»½,æ¯ä¸ªgroupå¯¹åº”N/Mä¸ªé€šé“,ç„¶åæ¯ä¸ªgroupå·ç§¯å®Œæˆåè¾“å‡ºå åœ¨ä¸€èµ·,ä½œä¸ºå½“å‰å±‚çš„è¾“å‡ºé€šé“ã€‚
â— ä»€ä¹ˆæ˜¯RNN
å‚è€ƒå›ç­”ï¼š
ä¸€ä¸ªåºåˆ—å½“å‰çš„è¾“å‡ºä¸å‰é¢çš„è¾“å‡ºä¹Ÿæœ‰å…³,åœ¨RNNç½‘ç»œç»“æ„ä¸­ä¸­,éšè—å±‚çš„è¾“å…¥ä¸ä»…åŒ…æ‹¬è¾“å…¥å±‚çš„è¾“å‡ºè¿˜åŒ…å«ä¸Šä¸€æ—¶åˆ»éšè—å±‚çš„è¾“å‡º,ç½‘ç»œä¼šå¯¹ä¹‹å‰çš„ä¿¡æ¯è¿›è¡Œè®°å¿†å¹¶åº”ç”¨äºå½“å‰çš„è¾“å…¥è®¡ç®—ä¸­ã€‚
â— è®­ç»ƒè¿‡ç¨‹ä¸­,è‹¥ä¸€ä¸ªæ¨¡å‹ä¸æ”¶æ•›,é‚£ä¹ˆæ˜¯å¦è¯´æ˜è¿™ä¸ªæ¨¡å‹æ— æ•ˆ?å¯¼è‡´æ¨¡å‹ä¸æ”¶æ•›çš„åŸå› æœ‰å“ªäº›?
å‚è€ƒå›ç­”ï¼š
å¹¶ä¸èƒ½è¯´æ˜è¿™ä¸ªæ¨¡å‹æ— æ•ˆ,å¯¼è‡´æ¨¡å‹ä¸æ”¶æ•›çš„åŸå› å¯èƒ½æœ‰æ•°æ®åˆ†ç±»çš„æ ‡æ³¨ä¸å‡†ç¡®,æ ·æœ¬çš„ä¿¡æ¯é‡å¤ªå¤§å¯¼è‡´æ¨¡å‹ä¸è¶³ä»¥fitæ•´ä¸ªæ ·æœ¬ç©ºé—´ã€‚å­¦ä¹ ç‡è®¾ç½®çš„å¤ªå¤§å®¹æ˜“äº§ç”Ÿéœ‡è¡,å¤ªå°ä¼šå¯¼è‡´ä¸æ”¶æ•›ã€‚å¯èƒ½å¤æ‚çš„åˆ†ç±»ä»»åŠ¡ç”¨äº†ç®€å•çš„æ¨¡å‹ã€‚æ•°æ®æ²¡æœ‰è¿›è¡Œå½’ä¸€åŒ–çš„æ“ä½œã€‚
â— å›¾åƒå¤„ç†ä¸­é”åŒ–å’Œå¹³æ»‘çš„æ“ä½œ
å‚è€ƒå›ç­”ï¼š
é”åŒ–å°±æ˜¯é€šè¿‡å¢å¼ºé«˜é¢‘åˆ†é‡æ¥å‡å°‘å›¾åƒä¸­çš„æ¨¡ç³Š,åœ¨å¢å¼ºå›¾åƒè¾¹ç¼˜çš„åŒæ—¶ä¹Ÿå¢åŠ äº†å›¾åƒçš„å™ªå£°ã€‚
å¹³æ»‘ä¸é”åŒ–ç›¸å,è¿‡æ»¤æ‰é«˜é¢‘åˆ†é‡,å‡å°‘å›¾åƒçš„å™ªå£°æ˜¯å›¾ç‰‡å˜å¾—æ¨¡ç³Šã€‚</p>
<p>â— VGGä½¿ç”¨3<em>3å·ç§¯æ ¸çš„ä¼˜åŠ¿æ˜¯ä»€ä¹ˆ?
å‚è€ƒå›ç­”ï¼š
2ä¸ª3</em>3çš„å·ç§¯æ ¸ä¸²è”å’Œ5<em>5çš„å·ç§¯æ ¸æœ‰ç›¸åŒçš„æ„ŸçŸ¥é‡,å‰è€…æ‹¥æœ‰æ›´å°‘çš„å‚æ•°ã€‚å¤šä¸ª3</em>3çš„å·ç§¯æ ¸æ¯”ä¸€ä¸ªè¾ƒå¤§å°ºå¯¸çš„å·ç§¯æ ¸æœ‰æ›´å¤šå±‚çš„éçº¿æ€§å‡½æ•°,å¢åŠ äº†éçº¿æ€§è¡¨è¾¾,ä½¿åˆ¤å†³å‡½æ•°æ›´å…·æœ‰åˆ¤å†³æ€§ã€‚
â— Reluæ¯”Sigmoidçš„æ•ˆæœå¥½åœ¨å“ªé‡Œ?
å‚è€ƒå›ç­”ï¼š
Sigmoidçš„å¯¼æ•°åªæœ‰åœ¨0çš„é™„è¿‘æ—¶æœ‰è¾ƒå¥½çš„æ¿€æ´»æ€§,è€Œåœ¨æ­£è´Ÿé¥±å’ŒåŒºåŸŸçš„æ¢¯åº¦è¶‹å‘äº0,ä»è€Œäº§ç”Ÿæ¢¯åº¦å¼¥æ•£çš„ç°è±¡,è€Œreluåœ¨å¤§äº0çš„éƒ¨åˆ†æ¢¯åº¦ä¸ºå¸¸æ•°,æ‰€ä»¥ä¸ä¼šæœ‰æ¢¯åº¦å¼¥æ•£ç°è±¡ã€‚Reluçš„å¯¼æ•°è®¡ç®—çš„æ›´å¿«ã€‚Reluåœ¨è´ŸåŠåŒºçš„å¯¼æ•°ä¸º0,æ‰€ä»¥ç¥ç»å…ƒæ¿€æ´»å€¼ä¸ºè´Ÿæ—¶,æ¢¯åº¦ä¸º0,æ­¤ç¥ç»å…ƒä¸å‚ä¸è®­ç»ƒ,å…·æœ‰ç¨€ç–æ€§ã€‚
â— é—®é¢˜ï¼šç¥ç»ç½‘ç»œä¸­æƒé‡å…±äº«çš„æ˜¯ï¼Ÿ
å‚è€ƒå›ç­”ï¼š
å·ç§¯ç¥ç»ç½‘ç»œã€å¾ªç¯ç¥ç»ç½‘ç»œ
è§£æï¼šé€šè¿‡ç½‘ç»œç»“æ„ç›´æ¥è§£é‡Š</p>
<p>â— é—®é¢˜ï¼šç¥ç»ç½‘ç»œæ¿€æ´»å‡½æ•°ï¼Ÿ
å‚è€ƒå›ç­”ï¼š
sigmodã€tanhã€relu
è§£æï¼šéœ€è¦æŒæ¡å‡½æ•°å›¾åƒï¼Œç‰¹ç‚¹ï¼Œäº’ç›¸æ¯”è¾ƒï¼Œä¼˜ç¼ºç‚¹ä»¥åŠæ”¹è¿›æ–¹æ³•</p>
<p>â— é—®é¢˜ï¼šåœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼Œé€šå¸¸ä¼šfinetuningå·²æœ‰çš„æˆç†Ÿæ¨¡å‹ï¼Œå†åŸºäºæ–°æ•°æ®ï¼Œä¿®æ”¹æœ€åå‡ å±‚ç¥ç»ç½‘ç»œæƒå€¼ï¼Œä¸ºä»€ä¹ˆï¼Ÿ
å‚è€ƒå›ç­”ï¼š
å®è·µä¸­çš„æ•°æ®é›†è´¨é‡å‚å·®ä¸é½ï¼Œå¯ä»¥ä½¿ç”¨è®­ç»ƒå¥½çš„ç½‘ç»œæ¥è¿›è¡Œæå–ç‰¹å¾ã€‚æŠŠè®­ç»ƒå¥½çš„ç½‘ç»œå½“åšç‰¹å¾æå–å™¨ã€‚
â— é—®é¢˜ï¼šç”»GRUç»“æ„å›¾
å‚è€ƒå›ç­”ï¼š</p>
<p>GRUæœ‰ä¸¤ä¸ªé—¨ï¼šæ›´æ–°é—¨ï¼Œè¾“å‡ºé—¨
è§£æï¼šå¦‚æœä¸ä¼šç”»GRUï¼Œå¯ä»¥ç”»LSTMæˆ–è€…RNNã€‚å†æˆ–è€…å¯ä»¥è®²è§£GRUä¸å…¶ä»–ä¸¤ä¸ªç½‘ç»œçš„è”ç³»å’ŒåŒºåˆ«ã€‚ä¸è¦ç›´æ¥å°±è¯´ä¸ä¼šã€‚</p>
<p>â— Attentionæœºåˆ¶çš„ä½œç”¨
å‚è€ƒå›ç­”ï¼š
å‡å°‘å¤„ç†é«˜ç»´è¾“å…¥æ•°æ®çš„è®¡ç®—è´Ÿæ‹…,ç»“æ„åŒ–çš„é€‰å–è¾“å…¥çš„å­é›†,ä»è€Œé™ä½æ•°æ®çš„ç»´åº¦ã€‚è®©ç³»ç»Ÿæ›´åŠ å®¹æ˜“çš„æ‰¾åˆ°è¾“å…¥çš„æ•°æ®ä¸­ä¸å½“å‰è¾“å‡ºä¿¡æ¯ç›¸å…³çš„æœ‰ç”¨ä¿¡æ¯,ä»è€Œæé«˜è¾“å‡ºçš„è´¨é‡ã€‚å¸®åŠ©ç±»ä¼¼äºdecoderè¿™æ ·çš„æ¨¡å‹æ¡†æ¶æ›´å¥½çš„å­¦åˆ°å¤šç§å†…å®¹æ¨¡æ€ä¹‹é—´çš„ç›¸äº’å…³ç³»ã€‚
â— Lstmå’ŒGruçš„åŸç†
å‚è€ƒå›ç­”ï¼š
Lstmç”±è¾“å…¥é—¨,é—å¿˜é—¨,è¾“å‡ºé—¨å’Œä¸€ä¸ªcellç»„æˆã€‚ç¬¬ä¸€æ­¥æ˜¯å†³å®šä»cellçŠ¶æ€ä¸­ä¸¢å¼ƒä»€ä¹ˆä¿¡æ¯,ç„¶ååœ¨å†³å®šæœ‰å¤šå°‘æ–°çš„ä¿¡æ¯è¿›å…¥åˆ°cellçŠ¶æ€ä¸­,æœ€ç»ˆåŸºäºç›®å‰çš„cellçŠ¶æ€å†³å®šè¾“å‡ºä»€ä¹ˆæ ·çš„ä¿¡æ¯ã€‚
Gruç”±é‡ç½®é—¨å’Œè·Ÿæ–°é—¨ç»„æˆ,å…¶è¾“å…¥ä¸ºå‰ä¸€æ—¶åˆ»éšè—å±‚çš„è¾“å‡ºå’Œå½“å‰çš„è¾“å…¥,è¾“å‡ºä¸ºä¸‹ä¸€æ—¶åˆ»éšè—å±‚çš„ä¿¡æ¯ã€‚é‡ç½®é—¨ç”¨æ¥è®¡ç®—å€™é€‰éšè—å±‚çš„è¾“å‡º,å…¶ä½œç”¨æ˜¯æ§åˆ¶ä¿ç•™å¤šå°‘å‰ä¸€æ—¶åˆ»çš„éšè—å±‚ã€‚è·Ÿæ–°é—¨çš„ä½œç”¨æ˜¯æ§åˆ¶åŠ å…¥å¤šå°‘å€™é€‰éšè—å±‚çš„è¾“å‡ºä¿¡æ¯,ä»è€Œå¾—åˆ°å½“å‰éšè—å±‚çš„è¾“å‡ºã€‚</p>
<p>â— ä»€ä¹ˆæ˜¯dropout
å‚è€ƒå›ç­”ï¼š
åœ¨ç¥ç»ç½‘ç»œçš„è®­ç»ƒè¿‡ç¨‹ä¸­,å¯¹äºç¥ç»å•å…ƒæŒ‰ä¸€å®šçš„æ¦‚ç‡å°†å…¶éšæœºä»ç½‘ç»œä¸­ä¸¢å¼ƒ,ä»è€Œè¾¾åˆ°å¯¹äºæ¯ä¸ªmini-batchéƒ½æ˜¯åœ¨è®­ç»ƒä¸åŒç½‘ç»œçš„æ•ˆæœ,é˜²æ­¢è¿‡æ‹Ÿåˆã€‚
â— LSTMæ¯ä¸ªé—¨çš„è®¡ç®—å…¬å¼
å‚è€ƒå›ç­”ï¼š
é—å¿˜é—¨:
è¾“å…¥é—¨:</p>
<p>è¾“å‡ºé—¨:</p>
<p>â— DropConnectçš„åŸç†
å‚è€ƒå›ç­”ï¼š
é˜²æ­¢è¿‡æ‹Ÿåˆæ–¹æ³•çš„ä¸€ç§,ä¸dropoutä¸åŒçš„æ˜¯,å®ƒä¸æ˜¯æŒ‰æ¦‚ç‡å°†éšè—å±‚çš„èŠ‚ç‚¹è¾“å‡ºæ¸…0,è€Œæ˜¯å¯¹æ¯ä¸ªèŠ‚ç‚¹ä¸ä¹‹ç›¸è¿çš„è¾“å…¥æƒå€¼ä»¥ä¸€å®šçš„æ¦‚ç‡æ¸…0ã€‚</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[å¹³å‡æ•°ç¼–ç ï¼šé’ˆå¯¹é«˜åŸºæ•°å®šæ€§ç‰¹å¾ï¼ˆç±»åˆ«ç‰¹å¾ï¼‰çš„æ•°æ®é¢„å¤„ç†/ç‰¹å¾å·¥ç¨‹]]></title>
        <id>https://xingyuezhiji.github.io//post/ping-jun-shu-bian-ma-zhen-dui-gao-ji-shu-ding-xing-te-zheng-lei-bie-te-zheng-de-shu-ju-yu-chu-li-te-zheng-gong-cheng</id>
        <link href="https://xingyuezhiji.github.io//post/ping-jun-shu-bian-ma-zhen-dui-gao-ji-shu-ding-xing-te-zheng-lei-bie-te-zheng-de-shu-ju-yu-chu-li-te-zheng-gong-cheng">
        </link>
        <updated>2019-03-14T02:43:27.000Z</updated>
        <content type="html"><![CDATA[<p>å¹³å‡æ•°ç¼–ç ï¼šé’ˆå¯¹é«˜åŸºæ•°å®šæ€§ç‰¹å¾ï¼ˆç±»åˆ«ç‰¹å¾ï¼‰çš„æ•°æ®é¢„å¤„ç†/ç‰¹å¾å·¥ç¨‹
å…‰å–»
å…‰å–»
æœºå™¨å­¦ä¹ /å› æœæ¨æ–­
â€‹å…³æ³¨ä»–
é±¼é‡é›¨æ¬²è¯­ä¸ä½™
ç­‰ 182 äººèµåŒäº†è¯¥æ–‡ç« 
ï¼ˆåœ¨å¦ä¸€ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘æ­£åœ¨æ±‡æ€»æ‰€æœ‰å·²çŸ¥çš„æ•°æ®æŒ–æ˜ç‰¹å¾å·¥ç¨‹æŠ€å·§ï¼šã€æŒç»­æ›´æ–°ã€‘æœºå™¨å­¦ä¹ ç‰¹å¾å·¥ç¨‹å®ç”¨æŠ€å·§å¤§å…¨ - çŸ¥ä¹ä¸“æ ã€‚ï¼‰</p>
<p>å‰è¨€</p>
<p>è¯»å®Œsklearn.preprocessingæ‰€æœ‰å‡½æ•°çš„APIæ–‡æ¡£ä¹‹åï¼ŒåŸºç¡€çš„ç‰¹å¾å·¥ç¨‹å°±å¯ä»¥ç®—æ˜¯å…¥é—¨äº†ã€‚ç„¶è€Œï¼Œè¿›é˜¶çš„ç‰¹å¾å·¥ç¨‹å¾€å¾€ä¾èµ–äºæ•°æ®åˆ†æå¸ˆçš„ç›´è§‰ä¸ç»éªŒï¼Œè€Œä¸”ä¸å…·ä½“çš„æ•°æ®æœ‰å¯†åˆ‡çš„è”ç³»ï¼Œæ¯”è¾ƒéš¾æ‰¾åˆ°ç³»ç»Ÿæ€§çš„â€œæœ€å¥½â€çš„ç‰¹å¾å·¥ç¨‹æ–¹æ³•ã€‚</p>
<p>åœ¨è¿™é‡Œï¼Œæˆ‘å¸Œæœ›èƒ½å‘å¤§å®¶åˆ†äº«ä¸€ç§æå…¶æœ‰æ•ˆçš„ã€é’ˆå¯¹é«˜åŸºæ•°å®šæ€§ç‰¹å¾ï¼ˆç±»åˆ«ç‰¹å¾ï¼‰çš„æ•°æ®é¢„å¤„ç†æ–¹æ³•ã€‚åœ¨å„ç±»ç«èµ›ä¸­ï¼Œæœ‰è®¸å¤šäººä½¿ç”¨è¿™ç§æ–¹æ³•å–å¾—äº†éå¸¸ä¼˜ç§€çš„æˆç»©ï¼Œä½†æ˜¯ä¸­æ–‡ç½‘ç»œä¸Šä¼¼ä¹è¿˜æ²¡æœ‰äººå¯¹æ­¤åšè¿‡ä»‹ç»ã€‚</p>
<p>å¹³å‡æ•°ç¼–ç ï¼šé’ˆå¯¹é«˜åŸºæ•°å®šæ€§ç‰¹å¾ï¼ˆç±»åˆ«ç‰¹å¾ï¼‰çš„æ•°æ®é¢„å¤„ç†
Mean Encoding: A Preprocessing Scheme for High-Cardinality Categorical Features
ï¼ˆè®ºæ–‡åŸæ–‡ä¸‹è½½ï¼šhttp://helios.mm.di.uoa.gr/~rouvas/ssi/sigkdd/sigkdd.vol3.1/barreca.pdfï¼Œæ„Ÿè°¢è¯„è®ºåŒº@jin zhangæä¾›æ›´æ¸…æ™°çš„pdfç‰ˆæœ¬ï¼‰</p>
<p>å¦‚æœæŸä¸€ä¸ªç‰¹å¾æ˜¯å®šæ€§çš„ï¼ˆcategoricalï¼‰ï¼Œè€Œè¿™ä¸ªç‰¹å¾çš„å¯èƒ½å€¼éå¸¸å¤šï¼ˆé«˜åŸºæ•°ï¼‰ï¼Œé‚£ä¹ˆå¹³å‡æ•°ç¼–ç ï¼ˆmean encodingï¼‰æ˜¯ä¸€ç§é«˜æ•ˆçš„ç¼–ç æ–¹å¼ã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™ç±»ç‰¹å¾å·¥ç¨‹èƒ½æå¤§æå‡æ¨¡å‹çš„æ€§èƒ½ã€‚</p>
<p>åœ¨æœºå™¨å­¦ä¹ ä¸æ•°æ®æŒ–æ˜ä¸­ï¼Œä¸è®ºæ˜¯åˆ†ç±»é—®é¢˜ï¼ˆclassificationï¼‰è¿˜æ˜¯å›å½’é—®é¢˜ï¼ˆregressionï¼‰ï¼Œé‡‡é›†çš„æ•°æ®å¸¸å¸¸ä¼šåŒ…æ‹¬å®šæ€§ç‰¹å¾ï¼ˆcategorical featureï¼‰ã€‚å› ä¸ºå®šæ€§ç‰¹å¾è¡¨ç¤ºæŸä¸ªæ•°æ®å±äºä¸€ä¸ªç‰¹å®šçš„ç±»åˆ«ï¼Œæ‰€ä»¥åœ¨æ•°å€¼ä¸Šï¼Œå®šæ€§ç‰¹å¾å€¼é€šå¸¸æ˜¯ä»0åˆ°nçš„ç¦»æ•£æ•´æ•°ã€‚ä¾‹å­ï¼šèŠ±ç“£çš„é¢œè‰²ï¼ˆçº¢ã€é»„ã€è“ï¼‰ã€æ€§åˆ«ï¼ˆç”·ã€å¥³ï¼‰ã€åœ°å€ã€æŸä¸€åˆ—ç‰¹å¾æ˜¯å¦å­˜åœ¨ç¼ºå¤±å€¼ï¼ˆè¿™ç§NA æŒ‡ç¤ºåˆ—å¸¸å¸¸ä¼šæä¾›æœ‰æ•ˆçš„é¢å¤–ä¿¡æ¯ï¼‰ã€‚</p>
<p>ä¸€èˆ¬æƒ…å†µä¸‹ï¼Œé’ˆå¯¹å®šæ€§ç‰¹å¾ï¼Œæˆ‘ä»¬åªéœ€è¦ä½¿ç”¨sklearnçš„OneHotEncoderæˆ–LabelEncoderè¿›è¡Œç¼–ç ï¼šï¼ˆdata_dfæ˜¯ä¸€ä¸ªpandas dataframeï¼Œæ¯ä¸€è¡Œæ˜¯ä¸€ä¸ªtraining exampleï¼Œæ¯ä¸€åˆ—æ˜¯ä¸€ä¸ªç‰¹å¾ã€‚åœ¨è¿™é‡Œæˆ‘ä»¬å‡è®¾&quot;street_address&quot;æ˜¯ä¸€ä¸ªå­—ç¬¦ç±»çš„å®šæ€§ç‰¹å¾ã€‚ï¼‰</p>
<p>from sklearn.preprocessing import OneHotEncoder, LabelEncoder
import numpy as np
import pandas as pd</p>
<p>le = LabelEncoder()
data_df['street_address'] = le.fit_transform(data_df['street_address'])</p>
<p>ohe = OneHotEncoder(n_values='auto', categorical_features='all', dtype=np.float64, sparse=True, handle_unknown='error')
one_hot_matrix = ohe.fit_transform(data_df['street_address'])
LabelEncoderèƒ½å¤Ÿæ¥æ”¶ä¸è§„åˆ™çš„ç‰¹å¾åˆ—ï¼Œå¹¶å°†å…¶è½¬åŒ–ä¸ºä»0åˆ°n-1çš„æ•´æ•°å€¼ï¼ˆå‡è®¾ä¸€å…±æœ‰nç§ä¸åŒçš„ç±»åˆ«ï¼‰ï¼›OneHotEncoderåˆ™èƒ½é€šè¿‡å“‘ç¼–ç ï¼Œåˆ¶ä½œå‡ºä¸€ä¸ªm*nçš„ç¨€ç–çŸ©é˜µï¼ˆå‡è®¾æ•°æ®ä¸€å…±æœ‰mè¡Œï¼Œå…·ä½“çš„è¾“å‡ºçŸ©é˜µæ ¼å¼æ˜¯å¦ç¨€ç–å¯ä»¥ç”±sparseå‚æ•°æ§åˆ¶ï¼‰ã€‚</p>
<p>æ›´è¯¦ç»†çš„APIæ–‡æ¡£å‚è§ï¼šsklearn.preprocessing.LabelEncoder - scikit-learn 0.18.1 documentationä»¥åŠsklearn.preprocessing.OneHotEncoder - scikit-learn 0.18.1 documentation</p>
<p>è¿™ç±»ç®€å•çš„é¢„å¤„ç†èƒ½å¤Ÿæ»¡è¶³å¤§å¤šæ•°æ•°æ®æŒ–æ˜ç®—æ³•çš„éœ€æ±‚ã€‚</p>
<p>å€¼å¾—ä¸€æçš„æ˜¯ï¼ŒLabelEncoderå°†nç§ç±»åˆ«ç¼–ç ä¸ºä»0åˆ°n-1çš„æ•´æ•°ï¼Œè™½ç„¶èƒ½å¤ŸèŠ‚çœå†…å­˜å’Œé™ä½ç®—æ³•çš„è¿è¡Œæ—¶é—´ï¼Œä½†æ˜¯éšå«äº†ä¸€ä¸ªå‡è®¾ï¼šä¸åŒçš„ç±»åˆ«ä¹‹é—´ï¼Œå­˜åœ¨ä¸€ç§é¡ºåºå…³ç³»ã€‚åœ¨å…·ä½“çš„ä»£ç å®ç°é‡Œï¼ŒLabelEncoderä¼šå¯¹å®šæ€§ç‰¹å¾åˆ—ä¸­çš„æ‰€æœ‰ç‹¬ç‰¹æ•°æ®è¿›è¡Œä¸€æ¬¡æ’åºï¼Œä»è€Œå¾—å‡ºä»åŸå§‹è¾“å…¥åˆ°æ•´æ•°çš„æ˜ å°„ã€‚</p>
<p>å®šæ€§ç‰¹å¾çš„åŸºæ•°ï¼ˆcardinalityï¼‰æŒ‡çš„æ˜¯è¿™ä¸ªå®šæ€§ç‰¹å¾æ‰€æœ‰å¯èƒ½çš„ä¸åŒå€¼çš„æ•°é‡ã€‚åœ¨é«˜åŸºæ•°ï¼ˆhigh cardinalityï¼‰çš„å®šæ€§ç‰¹å¾é¢å‰ï¼Œè¿™äº›æ•°æ®é¢„å¤„ç†çš„æ–¹æ³•å¾€å¾€å¾—ä¸åˆ°ä»¤äººæ»¡æ„çš„ç»“æœã€‚</p>
<p>é«˜åŸºæ•°å®šæ€§ç‰¹å¾çš„ä¾‹å­ï¼šIPåœ°å€ã€ç”µå­é‚®ä»¶åŸŸåã€åŸå¸‚åã€å®¶åº­ä½å€ã€è¡—é“ã€äº§å“å·ç ã€‚</p>
<p>ä¸»è¦åŸå› ï¼š</p>
<p>LabelEncoderç¼–ç é«˜åŸºæ•°å®šæ€§ç‰¹å¾ï¼Œè™½ç„¶åªéœ€è¦ä¸€åˆ—ï¼Œä½†æ˜¯æ¯ä¸ªè‡ªç„¶æ•°éƒ½å…·æœ‰ä¸åŒçš„é‡è¦æ„ä¹‰ï¼Œå¯¹äºyè€Œè¨€çº¿æ€§ä¸å¯åˆ†ã€‚ä½¿ç”¨ç®€å•æ¨¡å‹ï¼Œå®¹æ˜“æ¬ æ‹Ÿåˆï¼ˆunderfitï¼‰ï¼Œæ— æ³•å®Œå…¨æ•è·ä¸åŒç±»åˆ«ä¹‹é—´çš„åŒºåˆ«ï¼›ä½¿ç”¨å¤æ‚æ¨¡å‹ï¼Œå®¹æ˜“åœ¨å…¶ä»–åœ°æ–¹è¿‡æ‹Ÿåˆï¼ˆoverfitï¼‰ã€‚
OneHotEncoderç¼–ç é«˜åŸºæ•°å®šæ€§ç‰¹å¾ï¼Œå¿…ç„¶äº§ç”Ÿä¸Šä¸‡åˆ—çš„ç¨€ç–çŸ©é˜µï¼Œæ˜“æ¶ˆè€—å¤§é‡å†…å­˜å’Œè®­ç»ƒæ—¶é—´ï¼Œé™¤éç®—æ³•æœ¬èº«æœ‰ç›¸å…³ä¼˜åŒ–ï¼ˆä¾‹ï¼šSVMï¼‰ã€‚</p>
<p>å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥å°è¯•ä½¿ç”¨å¹³å‡æ•°ç¼–ç ï¼ˆmean encodingï¼‰çš„ç¼–ç æ–¹æ³•ï¼Œåœ¨è´å¶æ–¯çš„æ¶æ„ä¸‹ï¼Œåˆ©ç”¨æ‰€è¦é¢„æµ‹çš„åº”å˜é‡ï¼ˆtarget variableï¼‰ï¼Œæœ‰ç›‘ç£åœ°ç¡®å®šæœ€é€‚åˆè¿™ä¸ªå®šæ€§ç‰¹å¾çš„ç¼–ç æ–¹å¼ã€‚åœ¨Kaggleçš„æ•°æ®ç«èµ›ä¸­ï¼Œè¿™ä¹Ÿæ˜¯ä¸€ç§å¸¸è§çš„æé«˜åˆ†æ•°çš„æ‰‹æ®µã€‚</p>
<p>åŸºæœ¬æ€è·¯ä¸åŸç†ï¼š</p>
<p>å¹³å‡æ•°ç¼–ç æ˜¯ä¸€ç§æœ‰ç›‘ç£ï¼ˆsupervisedï¼‰çš„ç¼–ç æ–¹å¼ï¼Œé€‚ç”¨äºåˆ†ç±»å’Œå›å½’é—®é¢˜ã€‚ä¸ºäº†ç®€åŒ–è®¨è®ºï¼Œä»¥ä¸‹çš„æ‰€æœ‰ä»£ç éƒ½ä»¥åˆ†ç±»é—®é¢˜ä½œä¸ºä¾‹å­ã€‚</p>
<p>å‡è®¾åœ¨åˆ†ç±»é—®é¢˜ä¸­ï¼Œç›®æ ‡yä¸€å…±æœ‰Cä¸ªä¸åŒç±»åˆ«ï¼Œå…·ä½“çš„ä¸€ä¸ªç±»åˆ«ç”¨targetè¡¨ç¤ºï¼›æŸä¸€ä¸ªå®šæ€§ç‰¹å¾variableä¸€å…±æœ‰Kä¸ªä¸åŒç±»åˆ«ï¼Œå…·ä½“çš„ä¸€ä¸ªç±»åˆ«ç”¨kè¡¨ç¤ºã€‚</p>
<p>å…ˆéªŒæ¦‚ç‡ï¼ˆpriorï¼‰ï¼šæ•°æ®ç‚¹å±äºæŸä¸€ä¸ªtargetï¼ˆyï¼‰çš„æ¦‚ç‡ï¼ŒP(y = target)</p>
<p>åéªŒæ¦‚ç‡ï¼ˆposteriorï¼‰ï¼šè¯¥å®šæ€§ç‰¹å¾å±äºæŸä¸€ç±»æ—¶ï¼Œæ•°æ®ç‚¹å±äºæŸä¸€ä¸ªtargetï¼ˆyï¼‰çš„æ¦‚ç‡ï¼ŒP(target = y | variable = k)</p>
<p>æœ¬ç®—æ³•çš„åŸºæœ¬æ€æƒ³ï¼šå°†variableä¸­çš„æ¯ä¸€ä¸ªkï¼Œéƒ½è¡¨ç¤ºä¸ºï¼ˆä¼°ç®—çš„ï¼‰å®ƒæ‰€å¯¹åº”çš„ç›®æ ‡yå€¼æ¦‚ç‡ï¼š</p>
<p>\hat{P} (target = y | variable = k)ã€‚ï¼ˆä¼°ç®—çš„ç»“æœéƒ½ç”¨â€œ^â€è¡¨ç¤ºï¼Œä»¥ç¤ºåŒºåˆ†ï¼‰
ï¼ˆå¤‡æ³¨ï¼‰å› æ­¤ï¼Œæ•´ä¸ªæ•°æ®é›†å°†å¢åŠ ï¼ˆC-1ï¼‰åˆ—ã€‚æ˜¯C-1è€Œä¸æ˜¯Cçš„åŸå› ï¼š
\sum_{i}^{}{\hat{P}(target = y_i | variable = k)} =1ï¼Œæ‰€ä»¥æœ€åä¸€ä¸ªy_içš„æ¦‚ç‡å€¼å¿…ç„¶å’Œå…¶ä»–y_içš„æ¦‚ç‡å€¼çº¿æ€§ç›¸å…³ã€‚åœ¨çº¿æ€§æ¨¡å‹ã€ç¥ç»ç½‘ç»œä»¥åŠSVMé‡Œï¼Œä¸èƒ½åŠ å…¥çº¿æ€§ç›¸å…³çš„ç‰¹å¾åˆ—ã€‚å¦‚æœä½ ä½¿ç”¨çš„æ˜¯åŸºäºå†³ç­–æ ‘çš„æ¨¡å‹ï¼ˆgbdtã€rfç­‰ï¼‰ï¼Œä¸ªäººä»ç„¶ä¸æ¨èè¿™ç§over-parameterizationã€‚</p>
<p>å…ˆéªŒæ¦‚ç‡ä¸åéªŒæ¦‚ç‡çš„è®¡ç®—ï¼š</p>
<p>å› ä¸ºæˆ‘ä»¬æ²¡æœ‰ä¸Šå¸è§†è§’ï¼Œæ‰€ä»¥æˆ‘ä»¬åœ¨è®¡ç®—ä¸­ï¼Œéœ€è¦åˆ©ç”¨å·²æœ‰æ•°æ®ä¼°ç®—å…ˆéªŒæ¦‚ç‡å’ŒåéªŒæ¦‚ç‡ã€‚æˆ‘ä»¬åœ¨æ­¤ä½¿ç”¨çš„å…·ä½“æ–¹æ³•è¢«ç§°ä¸ºEmpirical Bayesï¼ˆEmpirical Bayes methodï¼‰ã€‚å’Œä¸€èˆ¬çš„è´å¶æ–¯æ–¹æ³•ï¼ˆå¦‚å¸¸è§çš„Laplace Smoothingï¼‰ä¸åŒï¼Œæˆ‘ä»¬åœ¨ä¼°ç®—å…ˆéªŒæ¦‚ç‡æ—¶ï¼Œä¼šä½¿ç”¨å·²çŸ¥æ•°æ®çš„å¹³å‡å€¼ï¼Œè€Œä¸æ˜¯\frac{1}{C} ã€‚</p>
<p>æ¥ä¸‹æ¥çš„è®¡ç®—å°±æ˜¯ç®€å•çš„ç»Ÿè®¡ï¼š</p>
<p>\hat{P}(y = target) = (y = target)çš„æ•°é‡ / y çš„æ€»æ•°</p>
<p>\hat{P}(target = y | variable = k) = (y = target å¹¶ä¸” variable = k)çš„æ•°é‡ / (variable = k)çš„æ•°é‡</p>
<p>ï¼ˆå…¶å®æˆ‘æœ¬æ¥å¯ä»¥æŠŠå…¬å¼ç”¨sigmaæ±‚å’Œå†™å‡ºæ¥çš„ï¼Œä½†æ˜¯é‚£æ ·ä¸å¤ªæ–¹ä¾¿ç›´è§‚ç†è§£ï¼‰</p>
<p>ä½¿ç”¨ä¸åŒçš„ç»Ÿè®¡æ–¹æ³•ï¼Œä»¥ä¸Šä¸¤ä¸ªå…¬å¼çš„è®¡ç®—æ–¹æ³•ä¹Ÿä¼šä¸åŒã€‚
æƒé‡ï¼š</p>
<p>æˆ‘ä»¬å·²ç»å¾—åˆ°äº†å…ˆéªŒæ¦‚ç‡ä¼°è®¡\hat{P}(y = target)å’ŒåéªŒæ¦‚ç‡ä¼°è®¡\hat{P}(target = y | variable = k)ã€‚æœ€ç»ˆç¼–ç æ‰€ä½¿ç”¨çš„æ¦‚ç‡ä¼°ç®—ï¼Œåº”å½“æ˜¯å…ˆéªŒæ¦‚ç‡ä¸åéªŒæ¦‚ç‡çš„ä¸€ä¸ªå‡¸ç»„åˆï¼ˆconvex combinationï¼‰ã€‚ç”±æ­¤ï¼Œæˆ‘ä»¬å¼•å…¥å…ˆéªŒæ¦‚ç‡çš„æƒé‡\lambda æ¥è®¡ç®—ç¼–ç æ‰€ç”¨æ¦‚ç‡\hat{P}ï¼š</p>
<p>\hat{P} = \lambda * \hat{P}(y = target) + (1 - \lambda) * \hat{P}(target = y | variable = k)
æˆ–ï¼š\hat{P} = \lambda * prior + (1 - \lambda) * posterior</p>
<p>ç›´è§‰ä¸Šï¼Œ\lambda åº”è¯¥å…·æœ‰ä»¥ä¸‹ç‰¹æ€§ï¼š</p>
<p>å¦‚æœæµ‹è¯•é›†ä¸­å‡ºç°äº†æ–°çš„ç‰¹å¾ç±»åˆ«ï¼ˆæœªåœ¨è®­ç»ƒé›†ä¸­å‡ºç°ï¼‰ï¼Œé‚£ä¹ˆ\lambda = 1ã€‚
ä¸€ä¸ªç‰¹å¾ç±»åˆ«åœ¨è®­ç»ƒé›†å†…å‡ºç°çš„æ¬¡æ•°è¶Šå¤šï¼ŒåéªŒæ¦‚ç‡çš„å¯ä¿¡åº¦è¶Šé«˜ï¼Œå…¶æƒé‡ä¹Ÿè¶Šå¤§ã€‚</p>
<p>åœ¨è´å¶æ–¯ç»Ÿè®¡å­¦ä¸­ï¼Œ\lambda ä¹Ÿè¢«ç§°ä¸ºshrinkage parameterã€‚
æƒé‡å‡½æ•°ï¼š</p>
<p>æˆ‘ä»¬éœ€è¦å®šä¹‰ä¸€ä¸ªæƒé‡å‡½æ•°ï¼Œè¾“å…¥æ˜¯ç‰¹å¾ç±»åˆ«åœ¨è®­ç»ƒé›†ä¸­å‡ºç°çš„æ¬¡æ•°nï¼Œè¾“å‡ºæ˜¯å¯¹äºè¿™ä¸ªç‰¹å¾ç±»åˆ«çš„å…ˆéªŒæ¦‚ç‡çš„æƒé‡\lambdaã€‚å‡è®¾ä¸€ä¸ªç‰¹å¾ç±»åˆ«çš„å‡ºç°æ¬¡æ•°ä¸ºnï¼Œä»¥ä¸‹æ˜¯ä¸€ä¸ªå¸¸è§çš„æƒé‡å‡½æ•°ï¼š</p>
<p>\lambda(n) = \frac{1}{1 + e^{(n - k)/f}}</p>
<p>è¿™ä¸ªå‡½æ•°æœ‰ä¸¤ä¸ªå‚æ•°ï¼š
kï¼šå½“n = kæ—¶ï¼Œ\lambda = 0.5ï¼Œå…ˆéªŒæ¦‚ç‡ä¸åéªŒæ¦‚ç‡çš„æƒé‡ç›¸åŒï¼›å½“n &gt; kæ—¶ï¼Œ\lambda &lt; 0.5ã€‚</p>
<p>fï¼šæ§åˆ¶å‡½æ•°åœ¨æ‹ç‚¹é™„è¿‘çš„æ–œç‡ï¼Œfè¶Šå¤§ï¼Œâ€œå¡â€è¶Šç¼“ã€‚</p>
<p>å›¾ç¤ºï¼šk=1æ—¶ï¼Œä¸åŒçš„få¯¹äºå‡½æ•°å›¾è±¡çš„å½±å“ã€‚</p>
<p>å½“(freq_col - k) / få¤ªå¤§çš„æ—¶å€™ï¼Œnp.expå¯èƒ½ä¼šäº§ç”Ÿoverflowçš„è­¦å‘Šã€‚æˆ‘ä»¬ä¸éœ€è¦ç®¡è¿™ä¸ªè­¦å‘Šï¼Œå› ä¸ºæŸä¸€ç±»åˆ«çš„é¢‘æ•°æé«˜ï¼Œåˆ†æ¯æ— é™æ—¶ï¼Œæœ€ç»ˆå…ˆéªŒæ¦‚ç‡çš„æƒé‡å°†æˆä¸º0ï¼Œè¿™ä¹Ÿè¡¨ç¤ºæˆ‘ä»¬å¯¹äºåéªŒæ¦‚ç‡æœ‰å……è¶³çš„ä¿¡ä»»ã€‚</p>
<p>å»¶ä¼¸</p>
<p>ä»¥ä¸Šçš„ç®—æ³•è®¾è®¡èƒ½è§£å†³å¤šä¸ªï¼ˆ&gt;2ï¼‰ç±»åˆ«çš„åˆ†ç±»é—®é¢˜ï¼Œè‡ªç„¶ä¹Ÿèƒ½è§£å†³æ›´ç®€å•çš„2ç±»åˆ†ç±»é—®é¢˜ä»¥åŠå›å½’é—®é¢˜ã€‚</p>
<p>è¿˜æœ‰ä¸€ç§æƒ…å†µï¼šå®šæ€§ç‰¹å¾æœ¬èº«åŒ…æ‹¬äº†ä¸åŒçº§åˆ«ã€‚ä¾‹å¦‚ï¼Œå›½å®¶åŒ…å«äº†çœï¼ŒçœåŒ…å«äº†å¸‚ï¼Œå¸‚åŒ…å«äº†è¡—åŒºã€‚æœ‰äº›è¡—åŒºå¯èƒ½å°±åŒ…å«äº†å¤§é‡çš„æ•°æ®ç‚¹ï¼Œè€Œæœ‰äº›çœå´å¯èƒ½åªæœ‰ç¨€å°‘çš„å‡ ä¸ªæ•°æ®ç‚¹ã€‚è¿™æ—¶ï¼Œæˆ‘ä»¬çš„è§£å†³æ–¹æ³•æ˜¯ï¼Œåœ¨empirical bayesé‡ŒåŠ å…¥ä¸åŒå±‚æ¬¡çš„å…ˆéªŒæ¦‚ç‡ä¼°è®¡ã€‚</p>
<p>ä»£ç å®ç°</p>
<p>åŸè®ºæ–‡å¹¶æ²¡æœ‰æåˆ°ï¼Œå¦‚æœfitæ—¶ä½¿ç”¨äº†å…¨éƒ¨çš„æ•°æ®ï¼Œtransformæ—¶ä¹Ÿä½¿ç”¨äº†å…¨éƒ¨æ•°æ®ï¼Œé‚£ä¹ˆä¹‹åçš„æœºå™¨å­¦ä¹ æ¨¡å‹ä¼šäº§ç”Ÿè¿‡æ‹Ÿåˆã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦å°†æ•°æ®åˆ†å±‚åˆ†ä¸ºn_splitsä¸ªfoldï¼Œæ¯ä¸€ä¸ªfoldçš„æ•°æ®éƒ½æ˜¯åˆ©ç”¨å‰©ä¸‹çš„(n_splits - 1)ä¸ªfoldå¾—å‡ºçš„ç»Ÿè®¡æ•°æ®è¿›è¡Œè½¬æ¢ã€‚n_splitsè¶Šå¤§ï¼Œç¼–ç çš„ç²¾åº¦è¶Šé«˜ï¼Œä½†ä¹Ÿæ›´æ¶ˆè€—å†…å­˜å’Œè¿ç®—æ—¶é—´ã€‚</p>
<p>ç¼–ç å®Œæ¯•åï¼Œæ˜¯å¦åˆ é™¤åŸå§‹ç‰¹å¾åˆ—ï¼Œåº”å½“å…·ä½“é—®é¢˜å…·ä½“åˆ†æã€‚</p>
<p>é™„ï¼šå®Œæ•´ç‰ˆMeanEncoderä»£ç ï¼ˆpythonï¼‰ã€‚</p>
<p>ä¸€ä¸ªMeanEncoderå¯¹è±¡å¯ä»¥æä¾›fit_transformå’Œtransformæ–¹æ³•ï¼Œä¸æ”¯æŒfitæ–¹æ³•ï¼Œæš‚ä¸æ”¯æŒè®­ç»ƒæ—¶çš„sample_weightå‚æ•°ã€‚</p>
<pre><code>
import numpy as np
import pandas as pd
from sklearn.model_selection import StratifiedKFold
from itertools import product

class MeanEncoder:
    def __init__(self, categorical_features, n_splits=5, target_type='classification', prior_weight_func=None):
        &quot;&quot;&quot;
        :param categorical_features: list of str, the name of the categorical columns to encode

        :param n_splits: the number of splits used in mean encoding

        :param target_type: str, 'regression' or 'classification'

        :param prior_weight_func:
        a function that takes in the number of observations, and outputs prior weight
        when a dict is passed, the default exponential decay function will be used:
        k: the number of observations needed for the posterior to be weighted equally as the prior
        f: larger f --&gt; smaller slope
        &quot;&quot;&quot;

        self.categorical_features = categorical_features
        self.n_splits = n_splits
        self.learned_stats = {}

        if target_type == 'classification':
            self.target_type = target_type
            self.target_values = []
        else:
            self.target_type = 'regression'
            self.target_values = None

        if isinstance(prior_weight_func, dict):
            self.prior_weight_func = eval('lambda x: 1 / (1 + np.exp((x - k) / f))', dict(prior_weight_func, np=np))
        elif callable(prior_weight_func):
            self.prior_weight_func = prior_weight_func
        else:
            self.prior_weight_func = lambda x: 1 / (1 + np.exp((x - 2) / 1))

    @staticmethod
    def mean_encode_subroutine(X_train, y_train, X_test, variable, target, prior_weight_func):
        X_train = X_train[[variable]].copy()
        X_test = X_test[[variable]].copy()

        if target is not None:
            nf_name = '{}_pred_{}'.format(variable, target)
            X_train['pred_temp'] = (y_train == target).astype(int)  # classification
        else:
            nf_name = '{}_pred'.format(variable)
            X_train['pred_temp'] = y_train  # regression
        prior = X_train['pred_temp'].mean()

        col_avg_y = X_train.groupby(by=variable, axis=0)['pred_temp'].agg({'mean': 'mean', 'beta': 'size'})
        col_avg_y['beta'] = prior_weight_func(col_avg_y['beta'])
        col_avg_y[nf_name] = col_avg_y['beta'] * prior + (1 - col_avg_y['beta']) * col_avg_y['mean']
        col_avg_y.drop(['beta', 'mean'], axis=1, inplace=True)

        nf_train = X_train.join(col_avg_y, on=variable)[nf_name].values
        nf_test = X_test.join(col_avg_y, on=variable).fillna(prior, inplace=False)[nf_name].values

        return nf_train, nf_test, prior, col_avg_y

    def fit_transform(self, X, y):
        &quot;&quot;&quot;
        :param X: pandas DataFrame, n_samples * n_features
        :param y: pandas Series or numpy array, n_samples
        :return X_new: the transformed pandas DataFrame containing mean-encoded categorical features
        &quot;&quot;&quot;
        X_new = X.copy()
        if self.target_type == 'classification':
            skf = StratifiedKFold(self.n_splits)
        else:
            skf = KFold(self.n_splits)

        if self.target_type == 'classification':
            self.target_values = sorted(set(y))
            self.learned_stats = {'{}_pred_{}'.format(variable, target): [] for variable, target in
                                  product(self.categorical_features, self.target_values)}
            for variable, target in product(self.categorical_features, self.target_values):
                nf_name = '{}_pred_{}'.format(variable, target)
                X_new.loc[:, nf_name] = np.nan
                for large_ind, small_ind in skf.split(y, y):
                    nf_large, nf_small, prior, col_avg_y = MeanEncoder.mean_encode_subroutine(
                        X_new.iloc[large_ind], y.iloc[large_ind], X_new.iloc[small_ind], variable, target, self.prior_weight_func)
                    X_new.iloc[small_ind, -1] = nf_small
                    self.learned_stats[nf_name].append((prior, col_avg_y))
        else:
            self.learned_stats = {'{}_pred'.format(variable): [] for variable in self.categorical_features}
            for variable in self.categorical_features:
                nf_name = '{}_pred'.format(variable)
                X_new.loc[:, nf_name] = np.nan
                for large_ind, small_ind in skf.split(y, y):
                    nf_large, nf_small, prior, col_avg_y = MeanEncoder.mean_encode_subroutine(
                        X_new.iloc[large_ind], y.iloc[large_ind], X_new.iloc[small_ind], variable, None, self.prior_weight_func)
                    X_new.iloc[small_ind, -1] = nf_small
                    self.learned_stats[nf_name].append((prior, col_avg_y))
        return X_new

    def transform(self, X):
        &quot;&quot;&quot;
        :param X: pandas DataFrame, n_samples * n_features
        :return X_new: the transformed pandas DataFrame containing mean-encoded categorical features
        &quot;&quot;&quot;
        X_new = X.copy()

        if self.target_type == 'classification':
            for variable, target in product(self.categorical_features, self.target_values):
                nf_name = '{}_pred_{}'.format(variable, target)
                X_new[nf_name] = 0
                for prior, col_avg_y in self.learned_stats[nf_name]:
                    X_new[nf_name] += X_new[[variable]].join(col_avg_y, on=variable).fillna(prior, inplace=False)[
                        nf_name]
                X_new[nf_name] /= self.n_splits
        else:
            for variable in self.categorical_features:
                nf_name = '{}_pred'.format(variable)
                X_new[nf_name] = 0
                for prior, col_avg_y in self.learned_stats[nf_name]:
                    X_new[nf_name] += X_new[[variable]].join(col_avg_y, on=variable).fillna(prior, inplace=False)[
                        nf_name]
                X_new[nf_name] /= self.n_splits

        return X_new
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[é«˜çº§ç‰¹å¾å·¥ç¨‹I]]></title>
        <id>https://xingyuezhiji.github.io//post/gao-ji-te-zheng-gong-cheng-i</id>
        <link href="https://xingyuezhiji.github.io//post/gao-ji-te-zheng-gong-cheng-i">
        </link>
        <updated>2019-03-06T15:04:48.000Z</updated>
        <content type="html"><![CDATA[<p>é«˜çº§ç‰¹å¾å·¥ç¨‹Iï»¿
Mean encodings
ä»¥ä¸‹æ˜¯Courseraä¸Šçš„How to Win a Data Science Competition: Learn from Top Kagglersè¯¾ç¨‹ç¬”è®°ã€‚
å­¦ä¹ ç›®æ ‡</p>
<p>Regularize mean encodings
Extend mean encodings
Summarize the concept of mean encodings
Concept of mean encoding
å‡å€¼ç¼–ç æ˜¯ä¸€ç§éå¸¸å¼ºå¤§çš„æŠ€æœ¯ï¼Œå®ƒæœ‰å¾ˆå¤šåå­—ï¼Œä¾‹å¦‚:likelihood encodingã€target encodingï¼Œä½†è¿™é‡Œæˆ‘ä»¬å«å®ƒå‡å€¼ç¼–ç ã€‚æˆ‘ä»¬ä¸¾ä¸€ä¸ªäºŒåˆ†ç±»ä»»åŠ¡çš„ä¾‹å­ã€‚
feature	feature_label	feature_mean	target
0	Moscow	1	0.4	0
1	Moscow	1	0.4	1
2	Moscow	1	0.4	1
3	Moscow	1	0.4	0
4	Moscow	1	0.4	0
5	Tver	2	0.8	1
6	Tver	2	0.8	1
7	Tver	2	0.8	1
8	Tver	2	0.8	0
9	Klin	0	0.0	0
10	klin	0	0.0	0
11	Tver	2	1	1
æˆ‘ä»¬æƒ³å¯¹featureå˜é‡è¿›è¡Œç¼–ç ï¼Œæœ€ç›´æ¥ã€å¸¸ç”¨çš„æ–¹å¼å°±æ˜¯label encodingï¼Œè¿™å°±æ˜¯ç¬¬äºŒåˆ—æ•°æ®ã€‚</p>
<p>å¹³å‡ç¼–ç ä»¥ä¸åŒçš„æ–¹å¼å»å®Œæˆè¿™ä¸ªä»»åŠ¡ï¼Œå®ƒç”¨æ¯ä¸ªåŸå¸‚è‡ªèº«å¯¹åº”çš„ç›®æ ‡å‡å€¼æ¥è¿›è¡Œç¼–ç ã€‚ä¾‹å¦‚ï¼Œå¯¹äºMoscowï¼Œæˆ‘ä»¬æœ‰äº”è¡Œï¼Œä¸‰ä¸ª0å’Œä¸¤ä¸ª1ã€‚ æ‰€ä»¥æˆ‘ä»¬ç”¨2é™¤ä»¥5æˆ–0.4å¯¹å®ƒè¿›è¡Œç¼–ç ã€‚ç”¨åŒæ ·çš„æ–¹æ³•å¤„ç†å…¶ä»–åŸå¸‚ã€‚
ç°åœ¨äº†è§£ä¸€ä¸‹ç»†èŠ‚ã€‚å½“æˆ‘ä»¬çš„æ•°æ®é›†éå¸¸å¤§ï¼ŒåŒ…å«æ•°ç™¾ä¸ªä¸åŒçš„åŸå¸‚ï¼Œè®©æˆ‘ä»¬è¯•ç€æ¯”è¾ƒä¸€ä¸‹ã€‚æˆ‘ä»¬ç»˜åˆ¶äº†0,1 classçš„ç›´æ–¹å›¾ã€‚
label_encoding.jpg
åœ¨label encodingçš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¾—åˆ°çš„å›¾çœ‹èµ·æ¥æ²¡æœ‰ä»»ä½•é€»è¾‘é¡ºåºã€‚
mean_encoding.jpg
ä½†æ˜¯å½“æˆ‘ä»¬ä½¿ç”¨mean encodingå¯¹ç›®æ ‡è¿›è¡Œç¼–ç æ—¶ï¼Œç±»çœ‹èµ·æ¥æ›´åŠ å¯åˆ†äº†ï¼Œåƒæ˜¯è¢«æ’åºè¿‡ã€‚
ä¸€èˆ¬æ¥è¯´ï¼Œæ¨¡å‹å¯¹å¤æ‚ã€éçº¿æ€§çš„ç‰¹å¾ç›®æ ‡è¶Šä¾èµ–ï¼Œå‡å€¼ç¼–ç è¶Šæœ‰æ•ˆã€‚ä¾‹å¦‚æ ‘æ¨¡å‹çš„æ·±åº¦æœ‰é™ï¼Œå¯ä»¥ç”¨å¹³å‡ç¼–ç æ¥è¡¥å¿å®ƒï¼Œå¯ä»¥ç”¨å®ƒçš„çŸ­æ¿æ¥è·å¾—æ›´å¥½çš„åˆ†æ•°ã€‚
ä»¥ä¸Šåªæ˜¯ä¸€ä¸ªä¾‹å­ï¼Œä¼ é€’çš„æ˜¯ä¸€ç§æ€æƒ³ï¼Œå®é™…ä¸Šå¯ä»¥åšå¾ˆå¤šç±»ä¼¼çš„æ“ä½œã€‚
Ways to use target variable</p>
<p>Goods-number of ones in a group,</p>
<p>Bads-number of zeros
Likelihood=GoodsGoods+Bads=mean(target)Likelihood=GoodsGoods+Bads=mean(target)
WeightofEvidence=ln(GoodsBads)âˆ—100WeightofEvidence=lnâ¡(GoodsBads)âˆ—100
Count=Goods=sum(target)Count=Goods=sum(target)
Diff=Goodsâˆ’BadsDiff=Goodsâˆ’Bads
æ„é€ Mean encodingçš„ä¾‹å­</p>
<p>
1
123</p>
<p>
1
means=X_tr.groupby(col).target.mean()train_new[col+'_mean_target']
=train_new[col].map(means)val_new[col+'_mean_target']=val_new[col].map
(means)
å°†å®ƒè¿ç”¨åˆ°æ¨¡å‹ä¸­ï¼Œå‡ºç°äº†ä¸¥é‡çš„è¿‡æ‹Ÿåˆï¼Œä½†æ˜¯ä¸ºä»€ä¹ˆå‘¢ï¼Ÿ
Train
feature	feature_label	feature_mean	target
8	Tver	2	0.8	0
9	Klin	0	0.0	0
Validation
feature	feature_label	feature_mean	target
10	klin	0	0.0	0
11	Tver	2	1	1
When they are categorized, itâ€™s pretty common to get results like in an example, target 0 in train and target 1 in validation. Mean encodings turns into a perfect feature for such categories. Thatâ€™s why we immediately get very good scores on train and fail hardly on validation.
Regularization
åœ¨ä¸Šä¸€èŠ‚ï¼Œæˆ‘ä»¬æ„è¯†åˆ°å¹³å‡ç¼–ç ä¸èƒ½æŒ‰åŸæ ·ä½¿ç”¨ï¼Œéœ€è¦å¯¹è®­ç»ƒæ•°æ®è¿›è¡ŒæŸç§æ­£è§„åŒ–ã€‚ç°åœ¨æˆ‘ä»¬å°†å®æ–½å››ç§ä¸åŒçš„æ­£åˆ™åŒ–æ–¹æ³•ã€‚
1.CV loop inside training data;
2.Smoothing;
3.Adding random noise;
4.Sorting and calculating expanding mean.
Conclusion</p>
<p>There are a lot ways to regularize mean encodings
Unending battle with target variable leakage
CV loop or Expanding mean for partical tasks.
1.KFold scheme</p>
<p>kfold.jpg
é€šå¸¸åšå››åˆ°äº”æŠ˜çš„äº¤å‰éªŒè¯å°±èƒ½å¾—åˆ°ä¸é”™çš„ç»“æœï¼Œæ— åºè°ƒæ•´æ­¤æ•°å­—ã€‚
ä»£ç ä¾‹å­
kfold_code.jpg
è¿™ä¸ªæ–¹æ³•çœ‹èµ·æ¥å·²ç»å®Œå…¨é¿å…äº†ç›®æ ‡å˜é‡çš„æ³„éœ²ï¼Œä½†äº‹å®å¹¶éå¦‚æ­¤ã€‚</p>
<p>è¿™é‡Œæˆ‘ä»¬é€šè¿‡ç•™ä¸€æ³•å¯¹Moscowè¿›è¡Œç¼–ç 
feature	feature_mean	target
0	Moscow	0.50	0
1	Moscow	0.25	1
2	Moscow	0.25	1
3	Moscow	0.50	0
4	Moscow	0.50	0
å¯¹äºç¬¬ä¸€è¡Œï¼Œæˆ‘ä»¬å¾—åˆ°0.5ï¼Œå› ä¸ºæœ‰ä¸¤ä¸ª1å’Œ å…¶ä½™è¡Œä¸­æœ‰ä¸¤ä¸ª0ã€‚ åŒæ ·ï¼Œå¯¹äºç¬¬äºŒè¡Œï¼Œæˆ‘ä»¬å¾—åˆ°0.25ï¼Œä¾æ­¤ç±»æ¨ã€‚ ä½†ä»”ç»†è§‚å¯Ÿï¼Œæ‰€æœ‰ç»“æœå’Œç”±æ­¤äº§ç”Ÿçš„ç‰¹å¾ã€‚ å®ƒå®Œç¾åœ°åˆ†å‰²æ•°æ®ï¼Œå…·æœ‰ç­‰äºæˆ–ç­‰çš„ç‰¹å¾çš„è¡Œ å¤§äº0.5çš„ç›®æ ‡ä¸º0ï¼Œå…¶ä½™è¡Œçš„ç›®æ ‡ä¸º1ã€‚ æˆ‘ä»¬æ²¡æœ‰æ˜ç¡®ä½¿ç”¨ç›®æ ‡å˜é‡ï¼Œä½†æˆ‘ä»¬çš„ç¼–ç æ˜¯æœ‰åç½®çš„ã€‚</p>
<p>ç›®æ ‡å˜é‡çš„æ³„éœ²æ•ˆæœå¯¹äºKFold schemeä»ç„¶æ˜¯æœ‰æ•ˆçš„ï¼Œåªæ˜¯æ•ˆæœæ¸©å’Œäº†ç‚¹ã€‚</p>
<p>åœ¨å®è·µä¸­ï¼Œå¦‚æœæ‚¨æœ‰è¶³å¤Ÿçš„æ•°æ®å¹¶ä½¿ç”¨å››æˆ–äº”æŠ˜ï¼Œç¼–ç å°†é€šè¿‡è¿™ç§æ­£è§„åŒ–ç­–ç•¥æ­£å¸¸å·¥ä½œã€‚ åªæ˜¯è¦å°å¿ƒå¹¶ä½¿ç”¨æ­£ç¡®çš„éªŒè¯ã€‚
2.Smoothing</p>
<p>Alpha controls the amount of regularization
Only works together with some other regularization method</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mfrac><mrow><mi>m</mi><mi>e</mi><mi>a</mi><mi>n</mi><mo>(</mo><mi>t</mi><mi>a</mi><mi>r</mi><mi>g</mi><mi>e</mi><mi>t</mi><mo>)</mo><mi>n</mi><mi>r</mi><mi>o</mi><mi>w</mi><mi>s</mi><mo>+</mo><mi>g</mi><mi>l</mi><mi>o</mi><mi>b</mi><mi>a</mi><mi>l</mi><mi>m</mi><mi>e</mi><mi>a</mi><mi>n</mi><mi>a</mi><mi>l</mi><mi>p</mi><mi>h</mi><mi>a</mi></mrow><mrow><mi>n</mi><mi>r</mi><mi>o</mi><mi>w</mi><mi>s</mi><mo>+</mo><mi>a</mi><mi>l</mi><mi>p</mi><mi>h</mi><mi>a</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">\frac{mean(target)nrows + globalmeanalpha}{nrows+alpha}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:2.30744em;vertical-align:-0.8804400000000001em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.427em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">n</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mord mathdefault">o</span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mord mathdefault">s</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord mathdefault">a</span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mord mathdefault">p</span><span class="mord mathdefault">h</span><span class="mord mathdefault">a</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">m</span><span class="mord mathdefault">e</span><span class="mord mathdefault">a</span><span class="mord mathdefault">n</span><span class="mopen">(</span><span class="mord mathdefault">t</span><span class="mord mathdefault">a</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="mord mathdefault">e</span><span class="mord mathdefault">t</span><span class="mclose">)</span><span class="mord mathdefault">n</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mord mathdefault">o</span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mord mathdefault">s</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mord mathdefault">o</span><span class="mord mathdefault">b</span><span class="mord mathdefault">a</span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mord mathdefault">m</span><span class="mord mathdefault">e</span><span class="mord mathdefault">a</span><span class="mord mathdefault">n</span><span class="mord mathdefault">a</span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mord mathdefault">p</span><span class="mord mathdefault">h</span><span class="mord mathdefault">a</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.8804400000000001em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p>
<p>å®ƒå…·æœ‰æ§åˆ¶æ­£åˆ™åŒ–é‡çš„è¶…å‚æ•°alphaã€‚ å½“alphaä¸ºé›¶æ—¶ï¼Œæˆ‘ä»¬æ²¡æœ‰æ­£åˆ™åŒ–ï¼Œå¹¶ä¸”å½“alphaæ¥è¿‘æ— ç©·å¤§æ—¶ï¼Œä¸€åˆ‡éƒ½å˜æˆäº†globalmeanã€‚
åœ¨æŸç§æ„ä¹‰ä¸Šï¼Œalphaç­‰äºæˆ‘ä»¬å¯ä»¥ä¿¡ä»»çš„ç±»åˆ«å¤§å°ã€‚ä¹Ÿå¯ä»¥ä½¿ç”¨å…¶ä»–ä¸€äº›å…¬å¼ï¼ŒåŸºæœ¬ä¸Šä»»ä½•æƒ©ç½šç¼–ç ç±»åˆ«çš„ä¸œè¥¿éƒ½å¯ä»¥è¢«è®¤ä¸ºæ˜¯smoothingã€‚
3.Nosie</p>
<p>Noise degrades the quality of encoding
é€šè¿‡æ·»åŠ å™ªå£°ï¼Œä¼šé™ä½è®­ç»ƒæ•°æ®çš„ç¼–ç è´¨é‡ã€‚è¿™ç§æ–¹æ³•å¾ˆä¸ç¨³å®šï¼Œå¾ˆéš¾ä½¿å®ƒå·¥ä½œã€‚ä¸»è¦é—®é¢˜åœ¨äºæˆ‘ä»¬éœ€è¦æ·»åŠ çš„å™ªå£°é‡ã€‚
How much noise should we add?
å¤ªå¤šçš„å™ªå£°ä¼šæŠŠè¿™ä¸ªç‰¹å¾å˜æˆåƒåœ¾ï¼Œè™½ç„¶å™ªå£°å¤ªå°æ„å‘³ç€æ›´æ­£è§„åŒ–ã€‚ä½ éœ€è¦åŠªåŠ›åœ°å¾®è°ƒå®ƒã€‚
Usually used together with LOO(Leave one out).
è¿™ç§æ–¹æ³•é€šå¸¸ä¸LOOæ­£åˆ™åŒ–ä¸€èµ·ä½¿ç”¨ã€‚å¦‚æœä½ æ²¡æœ‰å¾ˆå¤šæ—¶é—´ï¼Œå®ƒå¯èƒ½ä¸æ˜¯æœ€å¥½é€‰æ‹©ã€‚
4.Expanding mean</p>
<p>Least amount of leakage
No hyper parameters
Irregular encoding quality
Built-in in CatBoost.
ä»£ç ä¾‹å­</p>
<p>
1
123</p>
<p>
1
cumsum=df_tr.groupby(col)['target'].cumsum()-df_tr['target']cumcnt=df_tr
.groupby(col).cumcount()train_new[col+'_mean_target']=cusum/cumcnt</p>
<p>cumsumå­˜å‚¨ç›®æ ‡å˜é‡çš„ç´¯è®¡å’Œï¼Œç›´åˆ°ç»™å®šè¡Œï¼Œcumcntå­˜å‚¨ç´¯ç§¯è®¡æ•°ã€‚è¯¥æ–¹æ³•å¼•å…¥çš„ç›®æ ‡å˜é‡çš„æ³„æ¼é‡æœ€å°‘ï¼Œå”¯ä¸€çš„ç¼ºç‚¹æ˜¯ç‰¹å¾è´¨é‡ä¸å‡åŒ€ã€‚ä½†è¿™ä¸æ˜¯ä»€ä¹ˆå¤§ä¸äº†çš„äº‹ï¼Œæˆ‘ä»¬å¯ä»¥ä»ä¸åŒçš„æ•°æ®æ’åˆ—è®¡ç®—ç¼–ç çš„å¹³å‡æ¨¡å‹ã€‚
å®ƒè¢«ç”¨äºCatBooståº“ä¸­ï¼Œè¯æ˜äº†å®ƒåœ¨åˆ†ç±»æ•°æ®é›†ä¸Šè¡¨ç°éå¸¸å‡ºè‰²ã€‚
Extensions and generalizations
å¦‚ä½•åœ¨å›å½’å’Œå¤šåˆ†ç±»ä»»åŠ¡ä¸­è¿›è¡ŒMean encoding
å¦‚ä½•å°†ç¼–ç åº”ç”¨äºå…·æœ‰å¤šå¯¹å¤šå…³ç³»çš„é¢†åŸŸ
æˆ‘ä»¬å¯ä»¥æ ¹æ®æ—¶é—´åºåˆ—ä¸­çš„ç›®æ ‡æ„å»ºå“ªäº›åŠŸèƒ½
ç¼–ç äº¤äº’å’Œæ•°å­—ç‰¹å¾
Many-to-many relations</p>
<p>åŸå§‹æ•°æ®
User_id	APPS	Target
10	APP1;APP2;APP3	0
11	APP4;APP1	1
12	APP2	1
100	APP3;APP9	0
ç°åœ¨è€ƒè™‘ä¸€ä¸ªä¾‹å­ï¼ŒåŸºäºç”¨åœ¨æ™ºèƒ½æ‰‹æœºä¸Šå·²è£…çš„APPï¼Œé¢„æµ‹å®ƒæ˜¯å¦ä¼šå®‰è£…ï¼Œè¿™æ˜¯ä¸€ä¸ªäºŒåˆ†ç±»ä»»åŠ¡ã€‚ä»è¡¨ä¸­æ•°æ®å¯çŸ¥ï¼Œæ¯ä¸ªç”¨æˆ·å¯èƒ½æœ‰å¤šä¸ªåº”ç”¨ç¨‹åºï¼Œæ¯ä¸ªåº”ç”¨ç¨‹åºç”±å¤šä¸ªç”¨æˆ·ä½¿ç”¨ï¼Œå› æ­¤è¿™æ˜¯å¤šå¯¹å¤šçš„å…³ç³»ã€‚è€Œéº»çƒ¦åœ¨äºï¼Œå¦‚ä½•ä»å¤šå¯¹å¤šçš„å…³ç³»ä¸­æå–å‡å€¼ã€‚
é•¿æ•°æ®è¡¨ç¤º
User_id	APP_id	Target
10	APP1	0
10	APP2	0
10	APP3	0
11	APP4	1
11	APP1	1
æŠŠåŸå§‹æ•°æ®è½¬ä¸ºé•¿æ•°æ®è¡¨ç¤ºï¼Œå¦‚ä¸Šè¡¨ã€‚ä½¿ç”¨æ­¤è¡¨ï¼Œæˆ‘ä»¬å¯ä»¥è‡ªç„¶åœ°è®¡ç®—APPçš„å‡å€¼ç¼–ç ã€‚ä½†æ˜¯å¦‚ä½•å°†å…¶æ˜ å°„å›ç”¨æˆ·å‘¢ï¼Ÿ
æ¯ä¸ªç”¨æˆ·éƒ½æœ‰è®¸å¤šAPPï¼Œä½†ä¸éƒ½æ˜¯â€œAPP1,APP2,APP3â€ã€‚å› æ­¤æˆ‘ä»¬ç”¨å‘é‡è¡¨ç¤º(0.1,0.2,0.1)ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥ä»å‘é‡ä¸­æ”¶é›†å„ç§ç»Ÿè®¡æ•°æ®ï¼Œæ¯”å¦‚å‡å€¼ã€æ ‡å‡†å·®ã€æœ€å¤§æœ€å°å€¼ç­‰ç­‰ã€‚
Time series</p>
<p>Time structure allows us to make a lot of complicated features.
Rolling statistics of target variable.
ä¸€æ–¹é¢ï¼Œè¿™æ˜¯ä¸€ç§é™åˆ¶ï¼Œå¦ä¸€æ–¹é¢ï¼Œå®ƒå…è®¸æˆ‘ä»¬åªåšä¸€äº›å¤æ‚çš„ç‰¹å¾ã€‚è€ƒè™‘ä¸€ä¸ªä¾‹å­ï¼š
Day	User	Spend	Amount	Prev_user	Prev_spend_avg
1	101	FOOD	2.0	0.0	0.0
1	101	GAS	4.0	0.0	0.0
1	102	FOOD	3.0	0.0	0.0
2	101	GAS	4.0	6.0	4.0
2	101	TV	8.0	6.0	0.0
2	102	FOOD	2.0	3.0	2.5
æˆ‘ä»¬éœ€è¦é¢„æµ‹ç”¨æˆ·ä¼šä¸ºå“ªä¸ªç±»åˆ«èŠ±é’±ã€‚ æˆ‘ä»¬æœ‰ä¸¤å¤©çš„æ—¶é—´ï¼Œä¸¤ä¸ªç”¨æˆ·ï¼Œ å’Œä¸‰ä¸ªæ”¯å‡ºç±»åˆ«ã€‚ ä¸€äº›å¥½çš„ç‰¹å¾æ˜¯ç”¨æˆ·åœ¨å‰ä¸€å¤©æ¶ˆè´¹æ€»é¢ï¼Œæ‰€æœ‰ç”¨æˆ·åœ¨ç»™å®šç±»åˆ«ä¸­èŠ±è´¹çš„å¹³å‡é‡‘é¢ã€‚ å› æ­¤ï¼Œåœ¨ç¬¬1å¤©ï¼Œç”¨æˆ·101èŠ±è´¹6ç¾å…ƒï¼Œç”¨æˆ·102èŠ±è´¹$3ã€‚ å› æ­¤ï¼Œæˆ‘ä»¬è®¤ä¸ºè¿™äº›æ•°å­—æ˜¯ç¬¬2å¤©çš„æœªæ¥å€¼ã€‚ åŒæ ·ï¼Œå¯ä»¥æŒ‰ç±»åˆ«åˆ’åˆ†å¹³å‡é‡‘é¢ã€‚
æˆ‘ä»¬æ‹¥æœ‰çš„æ•°æ®è¶Šå¤šï¼Œå¯ä»¥åˆ›é€ çš„ç‰¹å¾å°±è¶Šå¤æ‚ã€‚
Interactions and numerical features</p>
<p>Analyzing fitted model
Binning numeric and selecting interactions
åœ¨å®è·µä¸­ï¼Œé€šå¸¸éœ€è¦ç¼–ç æ•°å­—ç‰¹å¾ä»¥åŠè¿›è¡Œç‰¹å¾ç»„åˆã€‚è¦å¯¹æ•°å­—ç‰¹å¾è¿›è¡Œç¼–ç ï¼Œæˆ‘ä»¬åªéœ€è¦å¯¹å…¶è¿›è¡Œåˆ†åŒºï¼Œç„¶åå°†å…¶è§†ä¸ºåˆ†ç±»ã€‚æˆ‘ä»¬ä»¥æ²¡æœ‰è¿›è¡Œä»»ä½•ç¼–ç çš„åŸå§‹ç‰¹å¾å’Œå†³ç­–æ ‘æ¨¡å‹ä¸ºä¾‹ã€‚
interaction_tree.png
å¦‚ä½•ä¸ºæ•°å­—ç‰¹å¾åˆ†ç»„ï¼Ÿ
å¦‚æœæ•°å­—ç‰¹å¾æœ‰å¾ˆå¤šåˆ†è£‚ç‚¹ï¼Œåˆ™è¡¨ç¤ºå®ƒäºç›®æ ‡æœ‰ä¸€äº›å¤æ‚çš„ä¾èµ–ï¼Œå¹¶ä¸”è¯•å›¾å»ç¼–ç å®ƒã€‚æ­¤å¤–è¿™äº›ç²¾ç¡®çš„åˆ†è£‚ç‚¹å¯ç”¨äºå¯¹ç‰¹å¾è¿›è¡Œåˆ†ç±»ï¼Œæ‰€ä»¥é€šè¿‡åˆ†ææ¨¡å‹ç»“æ„ï¼Œæˆ‘ä»¬æ—¢å¯ä»¥è¯†åˆ«è¿™äº›å¯ç–‘çš„æ•°å­—ç‰¹å¾ï¼Œåˆå¯ä»¥æ‰¾åˆ°å¾ˆå¥½çš„æ–¹æ³•å»ç»™å®ƒåˆ†ç»„ã€‚
å¦‚ä½•æŒ‘é€‰ç‰¹å¾ç»„åˆï¼Ÿ
å…ˆçœ‹å†³ç­–æ ‘ä¸­å¦‚ä½•æå–äº¤äº’ç‰¹å¾ã€‚å‚ç…§ä¸Šå›¾ï¼Œå¦‚æœä¸¤ä¸ªç‰¹å¾åœ¨ç›¸é‚»çš„èŠ‚ç‚¹ä¸­ï¼Œåˆ™è¿™ä¸¤ä¸ªç‰¹å¾åœ¨ç›¸äº’ä½œç”¨ã€‚è€ƒè™‘åˆ°è¿™ç‚¹ï¼Œæˆ‘ä»¬å¯ä»¥éå†æ¨¡å‹ä¸­çš„æ‰€æœ‰æ ‘ï¼Œè®¡ç®—æ¯ä¸ªç‰¹å¾ç»„åˆå‡ºç°çš„æ¬¡æ•°ã€‚æœ€å¸¸è§çš„ç»„åˆå¯èƒ½å€¼å¾—è¿›è¡Œå‡å€¼ç¼–ç ã€‚</p>
<p>ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬å‘ç°feature1å’Œfeature2è¿™ä¸€å¯¹ç‰¹å¾æœ€å¸¸è§ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨æ•°æ®ä¸­è¿æ¥è¿™äº›ç‰¹å¾ï¼Œè¿™æ„å‘³ç¼–ç äº§ç”Ÿäº¤äº’ã€‚
Correct validation reminder</p>
<p>Local experiments:
Estimate encodings on X_tr
Map them to X_tr and X_val
Regularize on X_tr
Validate model on X_tr/X_val split
Submission:
Estimate encodings on whole Train data
Map them on Train and Test
Regularize on Train
Fit on Train
reminder_set.png
End
Main advantages:
Compact transformation of categorical variables
Powerful basis for feature engineering
Disadvantages:
Need careful validation, there a lot of ways to overfit
Significant improvements only on specific datasets</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hello Hve Notes]]></title>
        <id>https://xingyuezhiji.github.io//post/hello-hve-notes</id>
        <link href="https://xingyuezhiji.github.io//post/hello-hve-notes">
        </link>
        <updated>2018-12-11T16:00:00.000Z</updated>
        <content type="html"><![CDATA[<p>ğŸ‘  æ¬¢è¿ä½¿ç”¨ <strong>Hve Notes</strong> ï¼</p>
<p>Github: <a href="https://github.com/hve-notes/hve-notes">Hve Notes</a><br>
é¡¹ç›®ä¸»é¡µ: <a href="http://hvenotes.fehey.com/">Hve Notes</a><br>
ç¤ºä¾‹ç½‘ç«™: <a href="http://fehey.com/">ç¤ºä¾‹ç½‘ç«™ä¸€</a> <a href="http://hve-notes.github.io">ç¤ºä¾‹ç½‘ç«™äºŒ</a></p>
<p>âœï¸  <strong>Hve Notes</strong> ä¸€ä¸ªé™æ€åšå®¢å†™ä½œå®¢æˆ·ç«¯ã€‚ä½ å¯ä»¥ç”¨å®ƒæ¥è®°å½•ä½ çš„ç”Ÿæ´»ã€å¿ƒæƒ…ã€çŸ¥è¯†ã€ç¬”è®°ã€åˆ›æ„... ...</p>
<h2 id="ç‰¹æ€§">ç‰¹æ€§ğŸ‘‡</h2>
<p>ğŸ“  ä½ å¯ä»¥ä½¿ç”¨æœ€é…·çš„ <strong>Markdown</strong> è¯­æ³•ï¼Œè¿›è¡Œå¿«é€Ÿåˆ›ä½œ</p>
<p>ğŸŒ‰  ä½ å¯ä»¥ç»™æ–‡ç« é…ä¸Šç²¾ç¾çš„å°é¢å›¾å’Œåœ¨æ–‡ç« ä»»æ„ä½ç½®æ’å…¥å›¾ç‰‡</p>
<p>ğŸ·ï¸  ä½ å¯ä»¥å¯¹æ–‡ç« è¿›è¡Œæ ‡ç­¾åˆ†ç»„</p>
<p>ğŸ“‹  ä½ å¯ä»¥è‡ªå®šä¹‰èœå•ï¼Œç”šè‡³å¯ä»¥åˆ›å»ºå¤–éƒ¨é“¾æ¥èœå•</p>
<p>ğŸ’»  ä½ å¯ä»¥åœ¨ <strong>ğ–¶ğ—‚ğ—‡ğ–½ğ—ˆğ—ğ—Œ</strong> æˆ– <strong>ğ–¬ğ–ºğ–¼ğ–®ğ–²</strong> è®¾å¤‡ä¸Šä½¿ç”¨æ­¤å®¢æˆ·ç«¯</p>
<p>ğŸŒ  ä½ å¯ä»¥ä½¿ç”¨ <strong>ğ–¦ğ—‚ğ—ğ—ğ—ğ–» ğ–¯ğ–ºğ—€ğ–¾ğ—Œ</strong> æˆ– <strong>Coding Pages</strong> å‘ä¸–ç•Œå±•ç¤ºï¼Œæœªæ¥å°†æ”¯æŒæ›´å¤šå¹³å°</p>
<p>ğŸ’¬  ä½ å¯ä»¥è¿›è¡Œç®€å•çš„é…ç½®ï¼Œæ¥å…¥ <a href="https://github.com/gitalk/gitalk">Gitalk</a> æˆ– <a href="https://github.com/SukkaW/DisqusJS">DisqusJS</a> è¯„è®ºç³»ç»Ÿ</p>
<p>ğŸ‡¬ğŸ‡§  ä½ å¯ä»¥ä½¿ç”¨<strong>ä¸­æ–‡ç®€ä½“</strong>æˆ–<strong>è‹±è¯­</strong></p>
<p>ğŸŒ  ä½ å¯ä»¥ä»»æ„ä½¿ç”¨åº”ç”¨å†…é»˜è®¤ä¸»é¢˜æˆ–ä»»æ„ç¬¬ä¸‰æ–¹ä¸»é¢˜</p>
<p>ğŸŒ± å½“ç„¶ <strong>Hve Notes</strong> è¿˜å¾ˆå¹´è½»ï¼Œæœ‰å¾ˆå¤šä¸è¶³ï¼Œä½†è¯·ç›¸ä¿¡ï¼Œå®ƒä¼šä¸åœå‘å‰ğŸƒ</p>
<p>æœªæ¥ï¼Œå®ƒä¸€å®šä¼šæˆä¸ºä½ ç¦»ä¸å¼€çš„ä¼™ä¼´</p>
<p>å°½æƒ…å‘æŒ¥ä½ çš„æ‰åå§ï¼</p>
<p>ğŸ˜˜ Enjoy~</p>
]]></content>
    </entry>
</feed>